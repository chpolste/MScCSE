Consider the double integrator dynamics

\startformula
    \VecX_{t+1} = \TwoByTwo{1}{1}{0}{1} \VecState_{t} + \TwoByOne{0.5}{1} \VecControl_{t} + \VecRandom_{t} \EndComma
\stopformula

where $\VecState_{t} \in \StateSpace = \ClosedInterval{-5}{5} \times \ClosedInterval{-3}{3}$, $\VecControl_{t} \in \ControlSpace = \ClosedInterval{-1}{1}$ and $\VecRandom_{t} \in \RandomSpace = \ClosedInterval{-0.1}{0.1}^2$.
The objective is to reach $\ClosedInterval{-1}{1}^2$, therefore the linear predicates $\Predicate_{1}$, $\Predicate_{2}$, $\Predicate_{3}$ and $\Predicate_{4}$ are introduced, corresponding to halfspaces governed by the inequalities $x \leq 1$, $-x \leq 1$, $y \leq 1$ and $-y \leq 1$.
The LTL formula expressing this reachability objective is then

\startformula
    \Finally ( p_1 \wedge p_2 \wedge p_3 \wedge p_4 )
\stopformula

with a co-safe interpretation.
\cite[Svorenova2017] used this exact system for their case studies and it is used here again as a baseline for comparison.

\placefigure[top][fig:cases-integrator-initial]{
    The double integrator test system and its initial partition.
    Polytopes $\State{1}$ to $\State{4}$ (grey) are outer states from the decomposition of $\ExtendedStateSpace \setminus \StateSpace$.
    The reachability target is $\State{12}$ (green).
}{
    \externalfigure[cases-integrator-initial][width=\textwidth]
}

Positive robust refinement and PreR, 1 iteration between analysis.
Illustrate limit behaviour at PreR edge.

Positive robust refinement with target expansion, 9 iterations.
Show super-small states.

Positive robust refinement with target expansion and small-state suppression, 9 iterations.
Show reduction of number of states.

Positive robust refinement with layer decomposition, no target expansion, shrunk PreR, small-state suppression, 3 inner iterations.
Motivate shrinking of PreR.
Show layer decomposition.
Faster refinement, few states, sub-minute analysis despite having to analyse the entire game.

Limit-behaviour at boundary of Outer Attr, therefore procedure can never terminate.
Proof?

Conclusion:
Layered refinement performs best, importance of small-state suppression (combats over-refinement) and problems with jaggedness (which just causes more jaggedness).
Plot number of state space partition elements vs. game size, show how product game simplification saves time?

\placetable[top][tab:cases-integrator-negative]{
    TODO
}{
    \RefinementTable{
        \RefinementTableRow[iteration=1,parts=9,onestates=28,oneactions=44,twostates=26,twoactions=216,
                            refinement={-},gamegraph={TODO},analysis={TODO},
                            yes=6.7,no=0,maybe=93.3,figure=cases-integrator-iteration1]
        \RefinementTableRow[iteration=2,parts=13,onestates=36,oneactions=60,twostates=38,twoactions=362,
                            refinement={TODO},gamegraph={TODO},analysis={TODO},
                            yes=6.7,no=11.3,maybe=82.1,figure=cases-integrator-iteration2]
        \RefinementTableRow[iteration=3,parts=17,onestates=44,oneactions=80,twostates=50,twoactions=592,
                            refinement={TODO},gamegraph={TODO},analysis={TODO},
                            yes=6.7,no=16.1,maybe=77.3,figure=cases-integrator-iteration3]
        \RefinementTableRow[iteration=4,parts=21,onestates=52,oneactions=104,twostates=66,twoactions=794,
                            refinement={TODO},gamegraph={TODO},analysis={TODO},
                            yes=6.7,no=17.2,maybe=76.2,figure=cases-integrator-iteration4]
    }
}

