Given a partition of state space, the associated game-graph and results of an analysis, how to determine what to refine and how?

Because analysis returns partial result, there is no need to refine states from $\YesStates{}$ or $\NoStates{}$.
Hence only states in $\MaybeStates{}$ should be the target of refinement.
Nature of the analysis from \in[abstraction-analysis] does not return specific counterexample traces through the game.

Game graph and dynamics of system used to guide refinement.
Game-based vs. geometric condition: game-based requires player 1 actions to be available (if all contain at least one no state in target set, condition is met).
While this is always the case after an analysis, for which the game is constructed, the condition can be expensive to evaluate if multiple refinement steps are taken between analysis because refinement invalidates the previous game graph at least partially.
Action construction for both players is of exponential complexity in number of reachable states and involves many geometric operations.
Alternative: purely geometric condition, use dynamics to detect situation that requires refinement without requiring the game graph.
Insight is somewhat restricted but condition might be much cheaper to evaluate with only a few geometric operations and polynomial complexity in the number of reachable states.

Ideal are refinement procedures with guaranteed progress, but this is not always possible.
Simple objectives such as reachability will provide immediate feedback to refinement after an analysis with the new partition, but complex objectives may require multiple refinements for different sub-objectives until the first states can be declared as satisfying.

Two basic objectives: enlarging set of yes states or enlarging set of no states.
\cite[Svorenova2017] called the refinement with the first goal \quotation{positive} and refinement with the second goal \quotation{negative}.
Here, the terms are expanded to the somewhat fuzzy meaning of positive refinement as everything that enables \quotation{good things} to happen, whereas negative refinement aims to avoid \quotation{bad things}.
These are sometimes overlapping categories.

Asymmetry in almost-sure analysis of LTL:
As long as anything bad can happen, objective cannot be fulfilled.
But good things only require a non-zero probability and enough tries and they will eventually happen.
LTL only cares about ordering, except for $\Next$, no operator has a time limit on how long things may take.
One can reasonably conclude that avoiding bad stuff is more important than ensuring good stuff.

If multiple refinement methods are available question is in which order should they be applied and how often and when should the game be analysed between refinements.
General answers are hard to find, as the best strategy for refinement depends on the objective, system dynamics and even on the notion of \quotation{best}.
Shortest time to result, quickest progress, smallest number of states, suitability for synthesis procedure, etc.
Automation is therefore complicated.

