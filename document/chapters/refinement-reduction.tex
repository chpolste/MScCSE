Holistic refinement procedures seek patterns in the entire product game graph.
The procedures presented in \in{Section}[sec:refinement-holistic] only consider a single step of the system dynamics due to the high computational cost of obtaining deeper insights into the game graph.
But without the consideration of multi-step behaviour, refinement progress is limited to a local scale.
Due to the high computational cost of construction and analysis of the product game, patterns suitable for non-local refinement are expensive to identify.
Computational demands can be reduced through the extraction of meaningful subsets of the game graph.
This is particularly interesting in the context of positive refinement.
Every satisfying player 1 strategy based on a subset of the product game can also be realized in the full product game as long as the power of player 2 has not been reduced by the subset selection.

One possible subset extraction method is to remove all but one action from every player 1 state of the game graph.
This subset selection only limits the abilities of player 1, so almost-sure satisfying strategies obtained from it can be transferred to the full game.
The reduction allows to remove player 1 from the game entirely so that it turns into an MDP.
The lower computational complexity of MDP analysis and uncontrolled dynamics improves the feasibility of multi-step analysis.
A refinement procedure based on this reduction can be constructed in the following way:
First a control region is assigned to every element of the state space partition.
This turns the controlled dynamcis of the LSS into piece-wise dynamics without control.
An MDP abstraction is constructed on top of the piece-wise system.
The state space is refined based on insights from the MDP.
Finally, the state space partition of the MDP system is transferred back to the original system.

Experience gathered with a prototype implementation of such a refinement scheme revealed drawbacks.
The choice of a control region for every state space partition element is non-trivial.
A certain amount of foresight is required in order to obtain an effective refinement procedure since badly chosen control regions significantly limit its potential.
The non-continuous piece-wise dynamics causes the refinement to produce jagged regions, which result in a very fine partitioning of the state space (see \in{Figure}[fig:refinement-transition-jagged]a where this issue is illustrated for positive robust refinement).
Such jaggedness quickly snowballs into an explosion of the number of (small) partition elements, which can offset the complexity reduction gained from the MDP abstraction.
Due to these issues, the approach was abandoned in favour of another subset extraction technique.

Instead of reducing the number of actions per state, one can directly reduce the number of states.
This makes sense when the remaining subset of states and actions forms a meaningful sub-problem of the entire product game.
It is not uncommon to encounter objectives made up of multiple separable sub-objectives, e.g.\ a safety property in conjunction with a recurrence objective, which is itself a sequence of reachability problems.
If these sub-objectives can be separated in the product game and solved individually, one can expect that a solution for the composite objective emerges.
For refinement, this is advantageous because the sub-problems will often provide more immediate analysis feedback after refinement.
The analysis of simple sub-problems might be possible without the need to construct a game abstraction altogether.
E.g., saftey properties can be verified with only the $\ActR$-operator.
Parallel refinement of the sub-problems is generally possible but restricted by the fact that state space partition is shared between the individual sub-problems in the product game.

