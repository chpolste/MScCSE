The first set of refinement procedures introduced is referred to as \quotation{holistic} methods because they operate on the entire product game.
They are single-step methods, i.e.\ they only take a single step of the LSS dynamics into account when evaluating which states to refine.
This restriction limits the complexity of patterns that can be identified but simplifies the procedures.


\startbuffer[buf:refinement-holistic-geometric-algorithm]
    \startalgorithmic[numbering=no,margin=0em]
        \INPUT{Element of state space partition $\State{i}$ }
        \OUTPUT{Partition of $\State{i}$}
    \stopalgorithmic
    \startalgorithmic
        \STATE{$Y \leftarrow \Set{\State{i}}$}
        \FORALL{$q \in Q$}
            \IF{$\State{i} \in \MaybeStates{q}$}
                \STATE{$q' \leftarrow \QNext{i}{q}$}
                \STATE{$Y' \leftarrow \emptyset$}
                \FORALL{$Y_n \in Y$}
                    \STATE{$Y' \leftarrow Y' \cup \RefinementStep{Y_n}{q'}$}
                \ENDFOR
                \STATE{$Y \leftarrow Y'$}
            \ENDIF
        \ENDFOR
        \RETURN{Y}
    \stopalgorithmic
\stopbuffer

\startsubsection[title={Robust Positive Refinement},reference=sec:refinement-holistic-positive]

    \placealgorithm[top][alg:refinement-holistic-positive]{
        A generic template for a single-step holistic refinement procedure based only on the system dynamics and analysis results.
        Implementations of $\RefineStep$ based on ideas from sections \in[sec:refinement-holistic-positive] (positive robust refinement), \in[sec:refinement-holistic-negattr] (negative attractor refinement) and \in[sec:refinement-holistic-safety] (safety refinement) are summarized in Table \in[tab:refinement-holistic-kernels].
    }{
        \startframedtext[width=\textwidth]
            \getbuffer[buf:refinement-holistic-geometric-algorithm]
        \stopframedtext
    }

    The first holistic refinement procedure is an adaptation of case 1 of the positive refinement proposed by \cite[Svorenova2017].
    It identifies states from which $P_\Yes$ can be reached robustly in a single step and uses the $\RefinePos$-kernel from section \in[sec:refinement-robust-kernel] to partition the associated state space polytopes.
    This can be done by looking for player 1 states in $P_\Maybe$ from which a state in $P_\Yes$ can be reached with probability 1 if both players cooperate, i.e.\ states $\Tuple{\State{i}}{q} \in P_\Maybe$ with an action $\PlayerOneAction{i}{J}$ leading to player 2 state $\Triple{\State{i}}{J}{q'}$, which has a player 2 action $\PlayerTwoAction{i}{J}{k}$ such that $\Tuple{\State{k}}{q'} \in P_\Yes$ for all $k \in K$.
    While this is a viable implementation of the refinement condition, it binds the procedure to the availability of player actions which could be associated with expensive game graph recomputation when the method is not used directly after an analysis (see section \in[sec:refinement-guidance]).

    Algorithm \in[alg:refinement-holistic-positive] demonstrates how the game graph construction can be avoided.
    Instead of analysing player actions, the algorithm systematically explores all player 1 states associated with a state space partition element $\State{i}$, filters out states that are not in $P_\Maybe$ (line 3) and determines the unique successor automaton state for all player 1 actions from the satisfied linear predicates (line 4).
    $\State{i}$ is then partitioned with a function $\RefineStep$ which, if chosen such that

    \startformula
        \RefinementStep{Y_n}{q'} = \RefinePositive{Y_n}{\YesStates{q'}} \EndComma
    \stopformula

    encapsulates both the desired condition and executes the refinement (lines 5 to 9).
    Because $\RefinePos$ can be implemented solely based on the LSS dynamics (Algorithm \in[alg:refinement-robust-kernel]), no dependence on the availability of the game graph exists.

    The properties of the robust refinement kernel guarantee that the procedure enlarges the state space volume associated with $P_\Yes$ whenever a state is successfully partitioned.
    However, if $P_\Yes$ is empty, no refinement can take place at all.
    This is not a problem for co-safe specifications where the final states are immediately recognized as satisfying states in the first analysis.
    For other objectives though, the initial partition will generally not yield any yes-states after the first analysis.
    This issue is addressed in section \in[sec:refinement-transition], where robust positive refinement will appear again in the context of a reachability decomposition approach.

\stopsubsection


\startsubsection[title={Negative Attractor},reference=sec:refinement-holistic-negattr]

    A second refinement condition identified by \cite[Svorenova2017] is the pattern 

    \placeformula[fml:refinement-holistic-negattr-condition]
    \startformula
        \startalign[n=2,align={right,left}]
            \NC \empty
            \NC \forall (\PlayerOneAction{i}{J}) \in Act \;\, \exists (\PlayerTwoAction{i}{J}{K}) \in Act \;\, \exists k \in K : \State{k} \in \NoStates{q'}
            \NR
            \NC \Leftrightarrow \quad
            \NC \forall (\PlayerOneAction{i}{J}) \in Act \;\, \exists j \in J : \State{j} \in \NoStates{q'} \EndComma
            \NR
        \stopalign
    \stopformula

    where $q' = \QNext{i}{q}$.
    It matches player 1 states in which player 2 can win the game with non-zero probability no matter which action player 1 chooses if playing as an adversary.
    For refinement, they proposed a negative procedure that partitions state space partition elements according to the attractor sets $\Attractor{\State{i}}{\ControlSpace}{\NoStates{q'}}$.
    Plays originating in this attractor region and transitioning to $q'$ are won by player 2 with non-zero probability inevitably, even if player 2 plays cooperatively.
    The removed attractor region is therefore be guaranteed to be recognized as non-satisfying in the next analysis.

    Again, it is possible to execute this refinement without having to consult the player actions of the game graph.
    Reusing the structure of algorithm \in[alg:refinement-holistic-positive], while choosing

    \startformula
        \startalign[n=2,align={right,left}]
            \NC \RefinementStep{Y_n}{q'} = 
            \NC \Convexify(\Attractor{Y}{\ControlSpace}{\NoStates{q'}})
            \NR
            \NC \empty
            \NC \quad \cup \Convexify(Y \setminus \Attractor{Y}{\ControlSpace}{\NoStates{q'}})
            \NR
        \stopalign
    \stopformula

    results in the negative refinement procedure of \cite[Svorenova2017].

\stopsubsection


\startbuffer[buf:refinement-holistic-positive-kernel]
    \startalgorithmic[numbering=no,margin=0em]
        \INPUT{Polytope $Y$, successor automaton state $q'$}
        \OUTPUT{Partition of $Y$}
    \stopalgorithmic
    \startalgorithmic
        \RETURN{$\RefinePositive{Y}{\YesStates{q'}}$}
    \stopalgorithmic
\stopbuffer

\startbuffer[buf:refinement-holistic-negattr-kernel]
    \startalgorithmic[numbering=no,margin=0em]
        \INPUT{Polytope $Y$, successor automaton state $q'$}
        \OUTPUT{Partition of $Y$}
    \stopalgorithmic
    \startalgorithmic
        \STATE{$A \leftarrow \Attractor{Y}{\ControlSpace}{\NoStates{q'}}$}
        \RETURN{$\Convexify(A) \cup \Convexify(Y \setminus A)$}
    \stopalgorithmic
\stopbuffer

\startbuffer[buf:refinement-holistic-safety-kernel]
    \startalgorithmic[numbering=no,margin=0em]
        \INPUT{Polytope $Y$, successor automaton state $q'$}
        \OUTPUT{Partition of $Y$}
    \stopalgorithmic
    \startalgorithmic
        \RETURN{$\RefinePositive{Y}{\StateSpace \setminus \NoStates{q'}}$}
    \stopalgorithmic
\stopbuffer

\startsubsection[title={Safety},reference=sec:refinement-holistic-safety]

    \placetable[top][tab:refinement-holistic-safety]{
        Implementations of the function $\RefineStep$ invoked by Algorithm \in[alg:refinement-holistic-positive] based on ideas from sections \in[sec:refinement-holistic-positive] (a, positive robust refinement), \in[sec:refinement-holistic-negattr] (b, negative attractor refinement) and \in[sec:refinement-holistic-safety] (c, safety refinement).
        All implementations only depend on the analysis results and the LSS dynamics but not the player actions of the product game graph.
    }{
        \startframedtext[width=\textwidth]
            a) \underbar{Positive robust refinement}
            \getbuffer[buf:refinement-holistic-positive-kernel]
        \stopframedtext
        \startframedtext[width=\textwidth,topframe=off]
            b) \underbar{Negative attractor refinement}
            \getbuffer[buf:refinement-holistic-negattr-kernel]
        \stopframedtext
        \startframedtext[width=\textwidth,topframe=off]
            c) \underbar{Safety refinement}
            \getbuffer[buf:refinement-holistic-safety-kernel]
        \stopframedtext
    }

    The negative attractor refinement presented in the previous section does not generally resolve all occurences of the problematic pattern (\in[fml:refinement-holistic-negattr-condition]).
    It only identifies and refines regions where player 2 wins inevitably with non-zero probability.
    A state $\Tuple{\State{i}}{q}$ of the product game might satisfy condition (\in[fml:refinement-holistic-negattr-condition]) but also have a player 1 action after whose selection states from $\NoStates{q'}$ with $q' = \QNext{i}{q}$ can be avoided almost-surely if player 2 cooperates, resulting in a safe transition from the perspective of player 1.
    In such cases, refinement should be applied that allows player 1 to enforce this safe transition even if player 2 plays as an adversary.
    A procedure of this kind belongs in the category of neutral refinement as it transfers control from player 2 to player 1 but neither aims to explicitly enlarge the state space volume associated with $P_\Yes$ nor the volume associated with $P_\No$.

    Algorithm \in[alg:refinement-holistic-positive] with the choice

    \startformula
        \RefinementStep{Y_n}{q'} = \RefinePositive{Y_n}{\StateSpace \setminus \NoStates{q'}}
    \stopformula

    results in a procedure that improves safety for player 1 by splitting state space partition elements associated with unsafe states with $\RefinePos$, targeted at a safe region.
    The procedure can only guarantee the existence of a safe player 1 action for some parts of the generated partition so multiple iterations might be necessary to ensure safety for all states.
    It is also possible that some regions of the state space cannot be made safe due to the \epsilon-limit behaviour seen in section \in[sec:abstraction-analysis-correctness].

    Table \in[tab:refinement-holistic-safety] summarizes the choices for the function $\RefineStep$ of Algorithm \in[alg:refinement-holistic-positive] from this and the previous two sections.

\stopsubsection


\startbuffer[buf:refinement-holistic-loops-algorithm]
    \startalgorithmic[numbering=no,margin=0em]
        \INPUT{Element of state space partition $\State{i}$}
        \OUTPUT{Partition of $\State{i}$}
    \stopalgorithmic
    \startalgorithmic
        \FORALL{$q \in Q$}
            \IF{$\State{i} \in \MaybeStates{q} \MidAnd \QNext{i}{q} = q$}
                \IF{$\exists J \ne \Set{i} : ( \PlayerTwoAction{i}{J}{\Set{i}} \in Act \MidAnd \IndexedStates{j}{J} \cap \NoStates{q} = \emptyset )$}
                    \STATE{$J$: witness from previous condition}
                    \STATE{$\ControlSpace_i^J \leftarrow \ConcreteAction{\State{i}}{\IndexedStates{j}{J}}$}
                    \STATE{$P \leftarrow \PrecisePredecessor{\State{i}}{\ControlSpace_i^J}{\State{i}}$}
                    \RETURN{$\Convexify(P) \cup \Convexify(\State{i} \setminus P)$}
                \ENDIF
            \ENDIF
        \ENDFOR
        \RETURN{$\Set{\State{i}}$}
    \stopalgorithmic
\stopbuffer

\startsubsection[title={Loop Removal},reference={sec:refinement-holistic-loops}]

    The motivation for neutral refinement was the stutter equivalence of plays when subjected to almost-sure analysis with respect to $\Next$-free LTL.
    In a neutral environment, player 1 can take any finite amount of turns to arrive at a good transition.
    But if an adversarial player 2 can force a play into an infinite, non-satisfying loop, then player 1 cannot win with probability 1 even if states from $P_\No$ are avoided almost-surely in every turn.
    Such loops in the game graph may involve multiple elements of the state space partition or multiple states of the automaton, in which case deep analysis of the product game is required for their identification.
    Much easier to find are self-loops, i.e.\ individual states of the game graph in which player 2 can trap a trace indefinitely.

    The same observation was made by \cite[Yordanov2012] in the context of hybrid systems with non-probabilistic, piecewise linear dynamics and LTL objectives.
    They separated self-loops of the abstraction into transient loops, which can be escaped in finite time in the original system by repeated application of the same control input, and non-transient loops, which cannot be broken by repeated application of the same control input.
    Their solution procedure did not incorporate abstraction refinement.
    Instead, they replaced non-transient self-loops in the game abstraction with corresponding non-looping player 1 actions, using the stutter equivalence property of $\Next$-free LTL.
    Here, instead of modifying the game graph pre-analysis, the problem of self-loops is approached with refinement.
    The goal is to refine the state space partition such that for all states of the game graph, player 1 has an action available after which player 2 is not able to enforce looping with probability 1.
    The refinement procedure in Algorithm \in[alg:refinement-holistic-loops] is proposed to remove potentially problematic self-loops in the product game graph.

    \placealgorithm[top][alg:refinement-holistic-loops]{
        Self-loop removal refinement for an element $\State{i}$ of the state space partition.
    }{
        \startframedtext[width=\textwidth,frame=off]
            \getbuffer[buf:refinement-holistic-loops-algorithm]
        \stopframedtext
    }

    Given an element $\State{i}$ of the state space partition, Algorithm \in[alg:refinement-holistic-loops] searches for an associated player 1 state $\Tuple{\State{i}}{q} \in \MaybeStates{q}$ which has a safe player 1 action $\PlayerOneAction{i}{J}$ that leads to a player 2 state $\Triple{\State{i}}{\PlayerOneAction{i}{J}}{q}$ in which a player 2 action $\PlayerTwoAction{i}{J}{\Set{i}}$ exists whose only target is the origin player 1 state $\Tuple{\State{i}}{q}$ again.
    The procedure only looks at player 1 states whose outgoing actions don't change the automaton state, otherwise stutter equivalence does not apply.
    Unsafe actions that lead to any state in $P_\No$ with non-zero probability are ignored as they are not considered to be neutral.
    The safety refinement of section \in[sec:refinement-holistic-safety] should be applied to these states first.
    Self-loops initiated deterministically by player 1 with an action $\PlayerOneAction{i}{\Set{i}}$ are also ignored since player 1 already has full control over the looping behaviour for these actions.

    If any player 2 action fulfills the self-loop condition, its corresponding origin polytope $\State{i}$ is partitioned according to the precise predecessor associated with the player 2 action.
    The precise predecessor set contains all states in $\State{i}$ from which a trace can potentially enter the self-loop.
    After extraction of these states, player 1 gains additional control over the looping behaviour while the trace localization power of player 2 is weakened.
    The refinement does not guarantee that the resulting states have no new self-loops, so multiple applications of the procedure may be required to ensure this.

    Note that if a state space part $\State{i}$ fulfills

    \startformula
        (\Posterior{\State{i}}{\ControlSpace} \cap \State{i}) \ominus \RandomSpace = \emptyset \EndComma
    \stopformula

    no self-loop can exist since any precise predecessor of $\Posterior{\State{i}}{\ControlSpace} \cap \State{i}$ will be empty.
    This property can be used to filter candidate states for the procedure relatively cheaply when the game graph has not been recomputed directly after another refinement step.
    It also ensures that all self-loop behaviour is eliminated eventually, namely when the property is satisfied by all states of the state space partition.

\stopsubsection

