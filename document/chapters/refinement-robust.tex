Section \in[sec:abstraction-operators] introduced 3 robust operators: $\PreR$, $\AttrR$ and $\ActR$.
The operators express aspects of the dynamics that are independent of the LSS's stochasticity and enforce transitions exclusively to the specified operator targets.
Robust operators are thus able provide guarantees in the context of refinement, enabling the design of procedures that ensure certain properties after their application.
This is important e.g.\ for safety objectives where the probability of visiting \quotation{bad} regions in a trace has to be 0 in order for the trace to be declared safe.

In positive refinement, robust dynamics can even deliver progress guarantees.
This was recognized by \cite[Svorenova2017] who designed a positive refinement procedure that uses the robust attractor and identifies regions from which a set of target states can be reached in a single step of the dynamics almost-surely.
If the target set is made up exclusively of states from $P_\Yes$, it is possible to satisfy the objective almost-surely from the $\AttrR$ regions.
Refinement with respect to these regions therefore guarantees that the state space volume associated with $P_\Yes$ is enlarged.
Before recreating this procedure in section \in[sec:refinement-holistic-positive], general properties of positive robust refinement need to be discussed.


\startbuffer[buf:refinement-robust-kernel-algorithm]
    \startalgorithmic[numbering=no,margin=0em]
        \INPUT{Origin polytope $Y \subseteq \StateSpace$, target region $Z \subseteq \StateSpace$ }
        \OUTPUT{Partition of $Y$}
    \stopalgorithmic
    \startalgorithmic
        \IF{$\RobustAction{Y}{Z} \neq \emptyset$}
            \RETURN{$Y$}
        \ENDIF
        \STATE{$\ControlRegion = TODO(Y, Z)$}
        \IF{$\ControlRegion = \emptyset$}
            \RETURN{$Y$}
        \ENDIF
        \STATE{$A = \RobustAttractor{Y}{\ControlRegion}{Z}$}
        \RETURN{$\Convexify(A) \cup \Convexify(Y \setminus A)$}
    \stopalgorithmic
\stopbuffer

\startsubsection[title={A Single-step Robust Refinement Kernel},sec:refinement-robust-kernel]

    \placealgorithm[top][alg:refinement-robust-kernel]{
        An implementation of $\RefinePos$ based on the robust attractor.
    }{
        \startframedtext[width=\textwidth,frame=off]
            \getbuffer[buf:refinement-robust-kernel-algorithm]
        \stopframedtext
    }

    The idea of positive refinement with robustness guarantee for a single step of the dynamics is encapsulated here in a positive robust refinement kernel

    \startformula
        \Function{\RefinePos}{C_n \times R_n}{2^{C_n}} \EndComma
    \stopformula

    where $R_n$ is the set of all (full-dimensional) polytopic regions in $\reals^n$ and $C_n$ is the set of all convex polytopes in $\reals^n$.
    $\RefinePositive{Y}{Z}$ returns a convex partition of the origin polytope $Y$ such that $Z$ can be reached exclusively and almost-surely after one step of the LSS dynamics from some non-empty set of elements of the partition of $Y$ for some non-empty control input region.
    This guarantees that after the refinement, player 1 has an action for this particular subset of $Y$ leading almost-surely and exclusively to $Z$ after just one turn of the game.
    If no subset of $Y$ fulfilling these properties can be determined or $Z$ can already be reached robustly from every state in $Y$, $Y$ is returned unchanged.

    This one-step positive refinement kernel is used as a building block for other, more complex refinement methods throughout this chapter.
    It finds application in the holistic positive refinement in section \in[sec:refinement-holistic-positive], the safety refinement of section \in[sec:refinement-holistic-safety] and the reachability procedures in section \in[sec:refinement-transition].

    $\RefinePos$ is implemented here based on the robust attractor refinement of \cite[Svorenova2017] in Algorithm \in[alg:refinement-robust-kernel].
    Lines 1-3 test if a robust transition is already possible.
    A control region is dermined in line 4 for which a robust transition to the target is possible for some states in $Y$.
    The implementation of function

    \startformula
        \Function{TODO}{R_n \times R_n}{R_m} \EndComma
    \stopformula

    where $m$ is the dimension of the control space, is discussed in the next section.
    If no such control region is found the origin polytope is returned unchanged (lines 5-7).
    Otherwise, the robust attractor with respect to the target region under control inputs from $\ControlRegion$ is computed and the origin partitoned accordingly.
    The function

    \startformula
        \Function{\Convexify}{R_n}{2^{C_n}}
    \stopformula

    partitions any polytopic region into a set of (disjunct) convex polytopes.
    Per definition, $\Convexify(\emptyset) = \emptyset$.

\stopsubsection


\startbuffer[buf:refinement-robust-control-algorithm]
    \startalgorithmic[numbering=no,margin=0em]
        \INPUT{Origin polytope $Y \subseteq \StateSpace$, target region $Z \subseteq \StateSpace$ }
        \OUTPUT{TODO}
    \stopalgorithmic
    \startalgorithmic
        \STATE{compute PreR} % TODO
        \IF{PreR is empty}
            \RETURN{$\emptyset$}
        \ENDIF
        \STATE{take vertices of PreR hull}
        \STATE{add random points from within PreR}
        \STATE{compute ActRs for every sample point}
        \STATE{cluster sample points}
        \RETURN{cluster that most points \quotation{agree} with}
    \stopalgorithmic
\stopbuffer

\startsubsection[title={Control Region Selection},reference=sec:refinement-robust-control]

    \placealgorithm[top][alg:refinement-robust-control]{
        TODO procedure to find control region for positive robust refinement
    }{
        \startframedtext[width=\textwidth,frame=off]
            \getbuffer[buf:refinement-robust-control-algorithm]
        \stopframedtext
    }

    The proposed implementation of $\RefinePos$ based on the $\AttrR$-operator requires the computation of a control region with respect to which the operator is calculated takes place (encapsulated in function $TODO$).
    In their positive refinement scheme, \cite[Svorenova2017] applied the robust attractor refinement to state space partition elements as the origin and target regions.
    Based on the player 1 actions of the corresponding origin game state, they selected a control region based on the $\ActC$-region of an action that exclusively leads to states in the target region.
    In order to obtain a non-empty $\AttrR$, they partitioned the $\ActC$-region arbitrarily and then refined with respect to one of the parts.
    However, for use in the $\RefinePos$ scheme this procedure is not ideal as the origin and target regions accepted as input can be arbitrary polytopic regions of the state space and are not bound to the current state space partition and game graph.

    An alternative approach, independent of the game abstraction, is presented in Algorithm \in[alg:refinement-robust-control].
    First sample points from within the target's $\PreR$ in the origin region, computed with respect to the entire control space.
    If a robust transition to the target region is possible, its origin must be contained inside the $\PreR$.
    Based on practical experience, adding the vertices of the hull of the $\PreR$-region to the sample pool improves refinement performance, as these points are often associated with \quotation{extreme} control input requirements.
    Using the $\ActR$-operator applied to these sample points, control regions are found that lead exclusively to the target.
    The control regions are then clustered and the region associated with the cluster that contains most state space sample vectors is selected.
    The idea is that the sample points are spread relatively uniformly over the polytope and the more points can utilize the same control region to make a robust transition, the larger the $\AttrR$-region will be.
    A rather expensive but exhaustive clustering method is to compute all overlaps of the individual control regions, similar to concrete action computation, and pick the one with the most contributung points.
    The number of sample points should grow with the dimensionality of the state space to ensure a representative selection is obtained.

\stopsubsection


\startsubsection[title={Limitations of Robust Refinement},reference=sec:refinement-robust-limitations]

    While the guarantees that robust refinement provides are advantageous, there are limitations to its usefulness.
    Robust refinement essentially treats the system as deterministic, looking only at transitions that are possible irrespective of the stochastic component.
    It is therefore a conservative view of the system, and neglects the positive potential of the probabilistic dynamics for player 1, treating the stochasticity as an adversary.
    Generally, this means that robust procedures tend to \quotation{over-refine} the state space, due to the strict requirements of robust dynamics.
    It also poses a limit to which problems can be treated, as the solution to some problems may unavoidably depend on the probabilistic dynamics.

    E.g., a robust transition is only possible if the target region has a non-empty robust predecessor, but this is not the case for targets $Z$ where $R \ominus W = \emptyset$ (section \in[sec:abstraction-operators-predecessor]).
    Even regions that can be targeted robustly but that are very small are a problem, as control has to be very precise and a sufficiently big target area has to be built up first, before well-performing refinement is possible.
    Target may be enlarged in these cases.
    This could be done by finding a superset $Z'$ around the target region $Z$ such that, so that the probability of going to $Z$ is non-zero for every trace that is steered into $Z'$, i.e.\ $(Z' \setminus Z) \ominus W = \emptyset$.
    The extended region $Z' \setminus Z$ may then require additional refinement, e.g.\ such that it fulfills a safety property with respect to $Z'$.

\stopsubsection

