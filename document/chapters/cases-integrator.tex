The double integrator case study of \cite[Svorenova2017] is replicated.
The system under consideration evolves according to the difference equation

\startformula
    \VecX_{t+1} = \TwoByTwo{1}{1}{0}{1} \VecState_{t} + \TwoByOne{0.5}{1} \VecControl_{t} + \VecRandom_{t} \EndComma
\stopformula

where $\VecState_{t} \in \StateSpace = \ClosedInterval{-5}{5} \times \ClosedInterval{-3}{3}$, $\VecControl_{t} \in \ControlSpace = \ClosedInterval{-1}{1}$ and $\VecRandom_{t} \in \RandomSpace = \ClosedInterval{-0.1}{0.1}^2$.
The problem is therefore mixed-dimensional, with a 2-dimensional state space and 1-dimensional control space.
The objective is to eventually reach the polytope $\ClosedInterval{-1}{1}^2$ almost-surely, i.e.\ a reachability problem with formula

\startformula
    \Finally ( \Predicate_{1} \wedge \Predicate_{2} \wedge \Predicate_{3} \wedge \Predicate_{4} ) \EndComma
\stopformula

where the linear predicates $\Predicate_{1}$, $\Predicate_{2}$, $\Predicate_{3}$ and $\Predicate_{4}$ correspond to halfspaces governed by the inequalities $x \leq 1$, $-x \leq 1$, $y \leq 1$ and $-y \leq 1$.
The co-safe interpretation is chosen and the reachability automaton from \in{Table}[tab:theory-logic-objectives] used.
The initial state space decomposition and the extended state space are visualized in \in{Figure}[fig:cases-integrator-initial].

\cite[Svorenova2017] have demonstrated that positive robust refinement is viable for this problem, so it is used here as a testbed for different configurations of robust refinement techniques.
Double integrator dynamics were also used as an example by \cite[AydinGol2015] but in a non-probabilistic setting.

\placefigure[top][fig:cases-integrator-initial]{
    The double integrator test system and its initial partition.
    Polytopes $\State{1}$ to $\State{4}$ (grey) are outer states from the decomposition of $\ExtendedStateSpace \setminus \StateSpace$.
    The reachability target is $\State{12}$ (green).
}{
    \externalfigure[cases-integrator-initial][width=\textwidth]
}


\startsubsection[title={Negative Refinement},reference=sec:cases-integrator-negative]

    \placetable[top][tab:cases-integrator-negative]{
        Initial analysis and negative attractor refinement for the double integrator system.
        The refinement procedure converged after 3 iterations.
        Polytopes in $\YesStates{q_0}$ are coloured green, polytopes in $\NoStates{q_0}$ grey and white polytopes are from $\MaybeStates{q_0}$.
        See \in{Section}[sec:cases-integrator-negative] for discussion.
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=1,polys=9,onestates=28,oneactions=44,twostates=26,twoactions=216,
                                total={0:00},refinement={-},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=0.0,maybe=93.3,figure=cases-integrator-iteration1]
            \RefinementTableRow[iteration=2,polys=13,onestates=36,oneactions=60,twostates=38,twoactions=362,
                                total={0:00},refinement={0:00},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=11.3,maybe=82.1,figure=cases-integrator-iteration2]
            \RefinementTableRow[iteration=3,polys=17,onestates=44,oneactions=80,twostates=50,twoactions=592,
                                total={0:01},refinement={0:00},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=16.1,maybe=77.3,figure=cases-integrator-iteration3]
            \RefinementTableRow[iteration=4,polys=21,onestates=52,oneactions=104,twostates=66,twoactions=794,
                                total={0:01},refinement={0:00},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=17.1,maybe=76.2,figure=cases-integrator-iteration4]
        }
    }

    Before utilizing positive refinement procedures, negative attractor refinement is applied.
    The initial negative region is made up of the outer polytopes of the system.
    3 iterations of the refinement-abstraction-analysis cycle are shown in \in{Table}[tab:cases-integrator-negative].
    As guaranteed by the refinement method, the attractor regions are immediately recognized as members of $\NoStates{q_0}$.
    After 3 iterations, the attractor has converged and no additional progress is made.

    In addition to a depiction of the state space partition after each refinement step in \in{Table}[tab:cases-integrator-negative], the elapsed time since problem initialization and the number of polytopes in the current state space partition are stated above the figures.
    Also reported are the size of the 2Â½-player game abstraction as player 1 and 2 state and action counts, the elapsed time during refinement application, game graph construction and analysis as well as the volume fraction of the regions $\YesStates{q_0}$ (yes), $\NoStates{q_0}$ (no) and $\MaybeStates{q_0}$ (maybe) with respect to the state space polytope $\StateSpace$.
    The table shows that refinement, abstraction and analysis in all iterations took less than 1 second.
    The initial analysis and the negative refinement required 1 second of run time in total.

\stopsubsection



\startsubsection[title={Positive Robust Refinement},reference=sec:cases-integrator-positive]

    \placetable[top][tab:cases-integrator-positive-single]{
        Refinement of transition $q_0 \rightarrow q_1$ of the double integrator system using 4 iterations of positive robust single-step refinement, applied twice in each iteration without robust target expansion.
        See \in{Section}[sec:cases-integrator-positive] for discussion.
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=48,onestates=106,oneactions=476,twostates=407,twoactions=6188,
                                total={0:04},refinement={0:00},gamegraph={0:03},analysis={0:00},
                                yes=13.9,no=17.1,maybe=68.9,figure=cases-integrator-iteration5-prs]
            \RefinementTableRow[iteration=6,polys=166,onestates=342,oneactions=3723,twostates=3519,twoactions=79332,
                                total={2:07},refinement={0:02},gamegraph={1:59},analysis={0:03},
                                yes=36.5,no=17.1,maybe=46.4,figure=cases-integrator-iteration6-prs]
            \RefinementTableRow[iteration=7,polys=301,onestates=612,oneactions=5902,twostates=5459,twoactions=111705,
                                total={5:59},refinement={0:09},gamegraph={3:40},analysis={0:03},
                                yes=54.9,no=17.1,maybe=28.0,figure=cases-integrator-iteration7-prs]
            \RefinementTableRow[iteration=8,polys=362,onestates=734,oneactions=3096,twostates=2458,twoactions=54578,
                                total={9:41},refinement={0:07},gamegraph={3:33},analysis={0:02},
                                yes=67.0,no=17.1,maybe=15.9,figure=cases-integrator-iteration8-prs]}
    }

    \placetable[top][tab:cases-integrator-positive-double]{
        Refinement of transition $q_0 \rightarrow q_1$ of the double integrator system using 4 iterations of positive robust two-step refinement with robust target expansion.
        See \in{Section}[sec:cases-integrator-positive] for discussion.
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=68,onestates=146,oneactions=885,twostates=796,twoactions=13251,
                                total={0:11},refinement={0:00},gamegraph={0:10},analysis={0:00},
                                yes=18.4,no=17.1,maybe=64.5,figure=cases-integrator-iteration5-prm]
            \RefinementTableRow[iteration=6,polys=236,onestates=482,oneactions=6293,twostates=6003,twoactions=141782,
                                total={4:32},refinement={0:06},gamegraph={4:08},analysis={0:06},
                                yes=53.9,no=17.1,maybe=29.0,figure=cases-integrator-iteration6-prm]
            \RefinementTableRow[iteration=7,polys=333,onestates=676,oneactions=4114,twostates=3567,twoactions=53441,
                                total={6:09},refinement={0:06},gamegraph={1:29},analysis={0:02},
                                yes=77.0,no=17.1,maybe=5.9,figure=cases-integrator-iteration7-prm]
            \RefinementTableRow[iteration=8,polys=361,onestates=732,oneactions=1367,twostates=682,twoactions=7436,
                                total={6:18},refinement={0:01},gamegraph={0:08},analysis={0:00},
                                yes=82.5,no=17.1,maybe=0.3,figure=cases-integrator-iteration8-prm]
        }
    }

    The first positive refinement procedure tested on the double integrator is configured to resemble that of \cite[Svorenova2017] (see their Chapter 5 and Figure 6).
    Their robust refinement kernel was driven by control regions selected based on the player 1 actions of the game and they applied the kernel 3 times between each analysis.
    The robust predecessor of the recognized yes-region was used to refine the system additionally.
    Here, only the kernel refinement is applied but with 2 applications between each analysis as the sampling-based control region selection is generally more effective than an action-based selection and progress per iteration is therefore similar.

    The progress from 4 iterations of refinement together with the familiar performance metrics are shown in \in{Table}[tab:cases-integrator-positive-single].
    They are labeled iterations 5 to 8 to indicate that the procedure continues after the negative refinement.
    The size of the state space partition doubles after the first iteration of refinement and then more than triples in the next iteration.
    A large amount of small states are created to the upper left and lower right of the target region.
    The asymmetry of the partitioning outside of the robust attractor region is due to the implementation of the geometric operations.
    Here, the partitioning in the lower right corner is favourable and leads to additional progress that is not seen in the upper left.
    This allows for faster progress in the upper right half of the state space in the 2 subsequent iterations whereas many small states are created in the upper left.
    The polygon count exceeds 300 after 3 iterations of positive refinement and increases only modestly by 61 in the last.
    After the final iteration, 84\% of the state space volume has been categorized as a subset of $\InitialStates$ or $\StateSpace \setminus \InitialStates$.
    The elapsed time is just under 10 minutes with most of the time spent in the construction of the game abstraction.

    Compared to the results of \cite[Svorenova2017] it can be noted that the amount of progress after 4 iterations is similar, but the partition generated here has more states.
    This can partially be attributed to the omission of the $\PreR$ refinement which seems to reduce overall jaggedness around the $\AttrR$-regions in their demonstration.

    The decomposition into robust reachability problems for specific automaton transitions enables multi-step refinement with an expanding target region.
    The double integrator problem is already a reachability problem and co-safe, therefore the decomposition is trivial here and the transition $q_0 \rightarrow q_1$ is chosen.
    A single-step method was applied twice between analyses in the previous experiment, now a two-step method is applied once.
    4 iterations of this refinement procedure are shown in \in{Table}[tab:cases-integrator-positive-double].
    Immediately in the first iteration, progress is faster than with the single-step scheme with 20 additional polytopes generated.
    The second iteration more than triples the partition size which leads to a costly game graph abstraction taking 4 minutes.
    Progress after this iteration however is comparable to that of the third iteration of the single-step method.
    The next 2 iterations grow the partition size by 125, with relatively short game graph construction times due to game simplification taking place based on the fast progress in the first 2 iterations (the number of game actions decreases after the second positive refinement).
    Only 0.3\% of the state space volume remain undecided after 4 iterations with an elapsed time that is more than 3 minutes shorter than that of the single-step method.

    \placetable[top][tab:cases-integrator-positive-suppressed]{
        Refinement of transition $q_0 \rightarrow q_1$ of the double integrator system using 4 iterations of positive robust two-step refinement with robust target expansion and small state suppression.
        See \in{Section}[sec:cases-integrator-positive] for discussion.
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=51,onestates=112,oneactions=507,twostates=435,twoactions=6147,
                                total={0:05},refinement={0:00},gamegraph={0:04},analysis={0:00},
                                yes=18.5,no=17.1,maybe=64.3,figure=cases-integrator-iteration5-prms]
            \RefinementTableRow[iteration=6,polys=99,onestates=208,oneactions=1360,twostates=1221,twoactions=21859,
                                total={0:26},refinement={0:02},gamegraph={0:18},analysis={0:01},
                                yes=42.4,no=17.1,maybe=40.4,figure=cases-integrator-iteration6-prms]
            \RefinementTableRow[iteration=7,polys=134,onestates=278,oneactions=1291,twostates=1069,twoactions=16174,
                                total={0:41},refinement={0:02},gamegraph={0:13},analysis={0:00},
                                yes=65.6,no=17.1,maybe=17.3,figure=cases-integrator-iteration7-prms]
            \RefinementTableRow[iteration=8,polys=161,onestates=332,oneactions=962,twostates=671,twoactions=8157,
                                total={0:48},refinement={0:01},gamegraph={0:05},analysis={0:00},
                                yes=80.1,no=17.1,maybe=2.7,figure=cases-integrator-iteration8-prms]
        }
    }

    The two-step refinement is applied again with two modifications:
    First, any state $Y$ with $Y \ominus \RandomSpace$ is not further refined unless the probability of reaching the no-region is non-zero for all control inputs.
    Second, the $\RefinePos$ partition is post-processed such that polytopes $Y$ where $Y \ominus \RandomSpace$ is empty are removed from the $\AttrR$-region, as shown in \in{Figure}[fig:refinement-transition-jagged](d).
    4 iterations of this modified refinement are presented in \in{Table}[tab:cases-integrator-positive-suppressed].
    As expected, progress per iteration is slower than without post-processing but the state space explosion is significantly reduced because the refinement is able to handle jaggedness without propagating it to smaller scales.
    After 4 iterations that take only a tenth of the time of the unmodified procedure, less than half the number of partition elements are generated and only 2.7\% of the system is left undecided.

    Similar partitioning is also achieved if a single iteration with an 8-step procedure is carried out (not shown), although the overall elapsed time is longer because the game construction cannot benefit from intermediate analysis results.
    Applying an overapproximating post-processing using the convex hull as shown in \in{Figure}[fig:refinement-transition-jagged](b) resulted in varying performance (not shown).
    While the overapproximation would sometimes work out and the system would continuously progress, the procedure could also get stuck due to the missing progress guarantee.

\stopsubsection


\startsubsection[title={Positive Robust Refinement with Layer Decomposition},reference=sec:cases-integrator-layered]

    \placetable[top][tab:cases-integrator-layered-original]{
        Refinement of transition $q_0 \rightarrow q_1$ of the double integrator reachability system using a layer decomposition generated by the robust predecessor and four-step positive robust refinement with robust target expansion small state suppression.
        See \in{Section}[sec:cases-integrator-layered] for discussion.
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=166,onestates=342,oneactions=4364,twostates=4177,twoactions=79056,
                                total={1:24},refinement={0:03},gamegraph={1:16},analysis={0:05},
                                yes=76.3,no=17.1,maybe=6.6,figure=cases-integrator-iteration5-lprms]
        }
    }

    The layer decomposition offers an alternative approach to multi-step refinement.
    It is now applied together with the post-processed, underapproximated robust refinement from the previous section.
    The robust predecessor is chosen as the layer-generating operator.
    Because reachability from the outer layers depends on the solution of the inner layers, up to 4 robust refinement steps are applied to each layer in order to increase the likelihood of successful refinement.

    \in{Table}[tab:cases-integrator-layered-original] shows the partitioning and performance metrics for a layered decomposition refinement.
    The $\PreR$ recurrence converged after 7 layers.
    The size of the resulting state space partition is similar to that of the final iteration of the post-processed two-step refinement but the elapsed time to solution is longer, since no intermediate analysis results are available for game simplification.
    It also fails to guarantee almost-sure reachability for some polytopes from the 2 outermost layers.
    This is likely related to the small polytope suppression as these polytopes are relatively long and thin and any further refinement is rejected by the post-processing.
    Long and thin states appear also along the edges of the layer boundaries.
    This is due to \epsilon-limit behaviour at the edge of the $\PreR$-regions.

    \placefigure[top][fig:cases-integrator-epsilon]{
        Illustration of the impact of \epsilon-limit behaviour (\in{Section}[sec:abstraction-analysis-correctness]) when using $\ControlSpace$ unmodified to generate layers for the double integrator example (left).
        The creation of long, thin states along the edges of the layers during the inner refinement of each layer can be combatted by slightly shrinking $\ControlSpace$ as demonstrated by the partition on the right.
    }{
        \startcombination[2*1]
            {\externalfigure[cases-integrator-layers-prer100][width=0.49\textwidth]}{}
            {\externalfigure[cases-integrator-layers-prer95][width=0.49\textwidth]}{}
        \stopcombination
    }

    It is possible to avoid this behaviour by shrinking the control region during the layer generation.
    \in{Figure}[fig:cases-integrator-epsilon] shows the effects of \epsilon-limit behaviour on the partition in the left frame.
    The panel on the right shows the same layered refinement but the control space was shrunk by 5\% during the generation of the layers, here from $\ClosedInterval{-1}{1}$ to $\ClosedInterval{-0.95}{0.95}$.
    This creates a \quotation{buffer zone} at the edges of the control space which allow the inner refinement steps to avoid the \epsilon-limit behaviour.
    Due to the smaller resulting $\PreR$-layers, an additional layer is required until the generator converges.
    The layers were shown in \in{Figure}[fig:refinement-transition-layers], which demonstrates that a small border between the outermost layer and the no-region remains due to the shrunk $\ControlSpace$.

    \in{Table}[tab:cases-integrator-layered-shrunk] shows the complete refinement results of this layer decomposition.
    Due to the elimination of the \epsilon-limit behaviour, 37 fewer states have been created compared to the unmodified $\PreR$ decomposition.
    The number of states that arise just from the layer decomposition alone is 106 (see \in{Figure}[fig:refinement-transition-layers]), meaning that 23 additional polytopes were generated to enable the desired layer-to-layer transitions.
    The number of player 1 and 2 actions in the game graph has been reduced significantly and this is reflected in a much smaller run time of the game construction.
    After the analysis, 99\% of the state space is decided with 29 seconds elapsed since initialization.
    This is faster than two-step refinement with post-processing which profited from game simplification due to intermediate analysis results being available.
    It is also the smallest state space partition of all positive robust refinement configurations tried.

    \placetable[top][tab:cases-integrator-layered-shrunk]{
        Refinement of transition $q_0 \rightarrow q_1$ of the double integrator reachability system using a layer decomposition generated by the robust predecessor with a 5\%-shrunk $\ControlSpace$ and four-step positive robust refinement with robust target expansion and small state suppression.
        See \in{Section}[sec:cases-integrator-layered] for discussion.
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=129,onestates=268,oneactions=2565,twostates=2415,twoactions=36246,
                                total={0:29},refinement={0:01},gamegraph={0:25},analysis={0:02},
                                yes=81.8,no=17.1,maybe=1.0,figure=cases-integrator-iteration5-slprms]
        }
    }

\stopsubsection


\startbuffer[buf:cases-integrator-results-statistics]
    \setupTABLE[frame=off,rightframe=on]
    \setupTABLE[c][first][rightframe=off]
    \setupTABLE[r][last][bottomframe=on]
    \bTABLE[align={middle,lohi}]
        \bTR[topframe=off]
            \bTH[width=0.06\textheight] \eTH
            \bTH[width=0.09\textheight] \eTH
            \bTH[width=0.17\textheight] single-step \times 2 \eTH
            \bTH[width=0.17\textheight] two-step \eTH
            \bTH[width=0.17\textheight] two-step \par small suppression \eTH
            \bTH[width=0.17\textheight] layers \par four-step \par small suppression \eTH
            \bTH[width=0.17\textheight] shrunk layers \par four-step \par small suppression \eTH
        \eTR
        \bTR[topframe=off]
            \bTH Iter. \eTH
            \bTH \eTH
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 1-4 \par neg. \eTH
            \bTD polytopes \eTD
            \bTD[nc=5] 21 / 21 / 21 \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD[nc=5] 0:01 / 0:01 / 0:01 \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD[nc=5] 23.8 / 23.8 / 23.8 \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 5 \eTH
            \bTD polytopes \eTD
            \bTD 48 / 50 / 53 \eTD
            \bTD 68 / 85 / 97 \eTD
            \bTD 49 / 54 / 60 \eTD
            \bTD 165 / 168 / 171 \eTD
            \bTD 128 / 130 / 133 \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 0:04 / 0:04 / 0:05 \eTD
            \bTD 0:11 / 0:21 / 0:36 \eTD
            \bTD 0:04 / 0:05 / 0:07 \eTD
            \bTD 1:19 / 1:24 / 1:27 \eTD
            \bTD 0:28 / 0:29 / 0:31 \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 30.7 / 31.0 / 31.4 \eTD
            \bTD 35.5 / 38.4 / 39.6 \eTD
            \bTD 35.5 / 36.7 / 39.2 \eTD
            \bTD 93.4 / 95.0 / 96.7 \eTD
            \bTD 99.0 / 99.0 / 99.0 \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 6 \eTH
            \bTD polytopes \eTD
            \bTD 166 / 194 / 239 \eTD
            \bTD 227 / 277 / 317 \eTD
            \bTD 83 / 99 / 114 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 1:55 / 4:00 / 11:56 \eTD
            \bTD 2:16 / 6:06 / 16:36 \eTD
            \bTD 0:18 / 0:25 / 0:36 \eTD
            \bTD - \eTD
            \bTD - \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 53.6 / 55.8 / 58.5 \eTD
            \bTD 70.7 / 74.8 / 79.4 \eTD
            \bTD 53.6 / 61.8 / 71.0 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 7 \eTH
            \bTD polytopes \eTD
            \bTD 239 / 280 / 321 \eTD
            \bTD 300 / 353 / 394 \eTD
            \bTD 124 / 132 / 137 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 4:04 / 7:14 / 17:13 \eTD
            \bTD 3:10 / 7:24 / 17:59 \eTD
            \bTD 0:26 / 0:41 / 0:50 \eTD
            \bTD - \eTD
            \bTD - \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 69.3 / 72.0 / 73.3 \eTD
            \bTD 87.8 / 95.6 / 99.5 \eTD
            \bTD 76.2 / 83.8 / 89.8 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3,bottomframe=on] 8 \eTH
            \bTD polytopes \eTD
            \bTD 296 / 351 / 408 \eTD
            \bTD 333 / 389 / 435 \eTD
            \bTD 141 / 151 / 163 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 6:15 / 12:35 / 27:24 \eTD
            \bTD 3:26 / 8:53 / 18:07 \eTD
            \bTD 0:31 / 0:47 / 0:58 \eTD
            \bTD - \eTD
            \bTD - \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 81.6 / 84.3 / 85.6 \eTD
            \bTD 95.5 / 99.2 / 99.8 \eTD
            \bTD 89.9 / 96.5 / 99.3 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
    \eTABLE
\stopbuffer

\startsubsection[title={Comparison and Discussion},reference=sec:cases-integrator-results]

    Statistics from 8 runs of the 5 presented refinement configurations are given in \in{Table}[tab:cases-integrator-results-statistics].
    The negative refinement was shared by all procedures, then the different methods were applied for 4 iterations or 1 iteration if layer decomposition was applied.
    The runs that were discussed in detail before are the (lower) median runs with respect to the elapsed time after the final iteration.

    The most striking observation from this table is the variability in partition size and elapsed time exhibited by the procedures not based on a layer decomposition.
    Randomization in the $\RefinePos$ kernel apparently has a significant effect on the refinement performance.
    Elapsed times for both refinement methods without post-processing vary by more than 15 minutes and the state space partition size by up to 100 polytopes.
    The variability of the non-layered procedure with small state suppression is much less, but still varies by half a minute.
    In contrast, the layered methods vary only by a few seconds and less than 10 polytopes in the state space partition.
    The results of the layer decomposition with shrunk control space show no variability at all in the volume of decided state space and less than 3 seconds of variability in the total elapsed time.
    One can conclude that the layer decomposition has a strong stabilizing effect on the refinement that counters the variability of the randomization of $\RefinePos$ and results in methods that deliver consistently good and dependable performance.

    % Rotation value has to be flipped for some reason (maybe because it refers
    % to the page that includes the figure, not the figure itself?
    % % TODO check rotation in final document
    \placetable[here,\doifoddpageelse{270}{90}][tab:cases-integrator-results-statistics]{
        Double integrator performance results for 5 configurations of the positive robust refinement procedure.
        Shown are the minimum, average and maximum number of polytopes in the $X$-partition, elapsed time after analysis and volume-percentage of the state space that has been identified as part of $\InitialStates$ or $\StateSpace \setminus \InitialStates$ from 8 runs of each procedure.
        Tables \in[tab:cases-integrator-negative] to \in[tab:cases-integrator-layered-shrunk] show the median runs with respect to total elapsed time in detail.
        Discussion in \in{Section}[sec:cases-integrator-results].
    }{
        \startframedtext[width=\textheight,offset=0mm,frame=off,topframe=off,bottomframe=off]
            \getbuffer[buf:cases-integrator-results-statistics]
        \stopframedtext
    }

    Missing in the implementation is the leveraging of robust progress guarantees by the game graph abstraction after robust refinement.
    This would speed up the methods further as robust reachability could be established without the need for abstraction.
    Player actions for many polytopes would then not have to be computed and game construction, which is the bottleneck of the entire solution scheme, would be cheaper.

    An issue that arose during the performance benchmark was numerical instability when operating on very small polytopes.
    3 runs of the single-step procedure had to be repeated due to a faulty game construction detected by the implementation.
    In every case the fault was a problem in the calculation of the $\PreP$-regions for extremely small polytopes.
    This is either an instability in the geometry library (not unthinkable considering that it is a custom development) and/or a general problem with polytope sizes near the smallest representable scale.
    Additional measures that avoid very fine partitioning could be implemented to combat this.
    Currently there is a size threshold under which states are not refined further but this does not stop the creation of very small states completely.
    Instead one could refuse any partitioning that produces a tiny polytope at the cost of refinement progress.

\stopsubsection

\startsubsection[title={Neutral Refinement},reference=sec:cases-integrator-neutral]

    \placetable[top][tab:cases-integrator-neutral]{
        Refinement of the double integrator reachability system with 2 applications of safety refinement followed by 5 applications of self-loop removal.
        See \in{Section}[sec:cases-integrator-neutral] for discussion.
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=201,onestates=412,oneactions=6660,twostates=6438,twoactions=142464,
                                total={4:04},refinement={0:36},gamegraph={3:14},analysis={0:13},
                                yes=80.8,no=17.1,maybe=2.1,figure=cases-integrator-iteration5-neutral]
        }
    }

    The discussion of the double integrator system is concluded by a demonstration of the effectiveness of neutral refinement techniques.
    After the negative refinement (\in{Table}[tab:cases-integrator-negative]), safety refinement is applied twice, followed by 5 iterations of self-loop removal without intermediate analysis.
    The indended purpose of the safety refinement is to contain the \epsilon-limit behaviour along the edges of the negative attractor regions, while the removal of self-loops should allow strategies to evolve traces inside the safe region until the target is eventually reached probabilistically.
    A partition of the state space resulting from such refinement is shown in \in{Table}[tab:cases-integrator-neutral] together with the familiar performance metrics.
    It is evident that the neutral refinement techniques alone are not competitive with layered refinement with respect to both run time and partition size.
    Notable is that the refinement took much longer than for any of the robust methods due to the self-loop refinement which requires the game graph to be (partially) constructed in between applications.
    The resulting partition is very fine around $y = 0$ and rather coarse for larger values of $|y|$.
    This can be explained by the dynamics of the double integrator, which has shear characteristics.
    The inherent motion of traces prescribed by the transformation with $\MatA$ is faster away from $y = 0$ where it vanishes entirely.
    Therefore, self-loops are more likely to occur near $y = 0$, where they can only be avoided through external control and a fine partition.

\stopsubsection

