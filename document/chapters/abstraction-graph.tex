To analyse the LSS, its infinite-member state- and control spaces must be replaced by finite counterparts and every trace realizable in the LSS must also be realizable in the resulting discrete abstraction.
Additionally, the abstraction may allow additional behaviour.
If a temporal logic specification is found to be satisfied by the abstraction, it will also be satisfied by the original system whose behaviour is a subset of the abstraction's.

The abstraction model chosen by \cite[Svorenova2017] is a probabilistic game graph $\GameGraph$ with 2-players, constructed from the dynamics operators from \in{Section}[sec:abstraction-operators].
In contrast to the derivation of \cite[Svorenova2017], the game is build directly here, without the definition of an intermediate non-deterministic transition system which is extending to a 2Â½-player game in order to reintroduce the stochasticity of the LSS.


\startsubsection[title={Player 1},reference=sec:abstraction-graph-playerone]

    The state-space discretization requires the grouping of all state-space vectors into a finite set of disjunct regions.
    As seen in \in{Section}[sec:theory-geometry], convex geometry has many advantageous computational properties, therefore a convex, polytopic partition of the extended state space $\ExtendedStateSpace$ is chosen.
    The inclusion of $\ExtendedStateSpace \setminus \StateSpace$ in addition to the state space simplifies the handling of transitions out of the state space during construction.
    In the abstraction-analysis-refinement iterations of the solution procedure (\in{Figure}[fig:problem-approach-flowchart]), this partition is subject to change.
    A sensible partition of $\StateSpace$ to initiate the procedure is based on the equivalence relation

    \placeformula[fml:abstraction-graph-decomposition]
    \startformula
        \VecState \sim_{\Predicates} \VecState' \;\Longleftrightarrow\; \PredicatesOf{\VecState} = \PredicatesOf{\VecState'} \EndComma
    \stopformula

    induced by the linear predicates $\Predicates$, where $\Function{\FulfilledPredicates}{\StateSpace}{2^\Predicates}$ associates a state vector with the predicates it fulfills.
    The partition is polytopic, convex and allows a straightforward connection to the temporal logic specification later.
    The region $\ExtendedStateSpace \setminus \StateSpace$ can be partitioned arbitrarily into convex polytopes.
    It only exists as a convenient transition target and is not subjected to refinement.

    The player 1 states

    \startformula
        G_1 = \IndexedStates{i}{I}
    \stopformula

    of the game abstraction $\GameGraph$ are based on this decomposition of $\ExtendedStateSpace$.
    These states are directly identified with the corresponding polytopes of the state-space partition, which are enumerated by the index set $I$.
    In a geometric context $\State{i}$ refers to the polytope while in a game-theoretic context $\State{i}$ refers to the associated player 1 state.

    \placefigure[top][fig:abstraction-graph-partition]{
        The initial state space partition for the example system (\in[fml:abstraction-example]), induced by the equivalence relation $\sim_{\Set{\Predicate_0}}$ where $\Predicate_0$ is a linear predicate associated with halfspace $\Set{ \Vec{x} \in \reals^n \mid -\VecX \leq 2 }$.
        $\State{1}$ and $\State{2}$ are the outer parts from the decomposition of $\ExtendedStateSpace \setminus \StateSpace$.
    }{
        % Put in wide box so that figure caption has full width
        \framed[width=\textwidth,frame=off]{\externalfigure[abstraction-graph-partition][width=0.7\textwidth]}
    }

    \in{Figure}[fig:abstraction-graph-partition] shows the initial state space partition for the example system (\in[fml:abstraction-example]) when equipped with a linear predicate $\Predicate_0$ associated with the halfspace $ \Set{ \Vec{x} \in \reals^n \mid -\VecX \leq 2 } $.
    The equivalence relation $\sim_{\Set{\Predicate_0}}$ partitions the state space into two parts $\State{3} = \ClosedInterval{2}{4}$ and $\State{4} = \ClosedInterval{0}{2}$ such that $\PredicatesOf{\VecState} = \Set{\Predicate_0}$ for all $\VecState \in \State{3}$ and $\PredicatesOf{\VecState} = \emptyset$ for all $\VecState \in \State{4}$.
    Two additional outer polytopes $\State{1} = \ClosedInterval{-1.1}{0}$ and $\State{2} = \ClosedInterval{4}{5.1}$ arise from the convex decomposition of $\ExtendedStateSpace \setminus \StateSpace$.

    The state space partition induces an equivalence relation $\sim_\VecState$ over the control space for any given state vector $\VecState$ with

    \startformula
        \VecControl \sim_\VecState \VecControl' \;\Longleftrightarrow\;
        \forall j \in \StateIndices: \Big(
            ( \Posterior{\VecState}{\VecControl} \cap \State{j} = \emptyset ) \,\leftrightarrow\,
            ( \Posterior{\VecState}{\VecControl'} \cap \State{j} = \emptyset )
        \Big) \EndComma
    \stopformula

    i.e.\ two control vectors are equivalent if and only if the same set of state space partition elements is reachable under them after one step of system evolution when starting in $\VecState$.
    In order to obtain actions for a state $\State{i}$ based on this relation it must be extended to a polytope-based notion.
    However, not all states in a given $\State{i}$ necessarily produce the same relation, so a common $\sim_{\State{i}}$ does not emerge naturally.
    Instead, it is defined as

    \startformula
        \VecControl \sim_{\State{i}} \VecControl' \;\Longleftrightarrow\;
        \forall j \in \StateIndices: \Big(
            ( \Posterior{\State{i}}{\VecControl} \cap \State{j} = \emptyset ) \,\leftrightarrow\,
            ( \Posterior{\State{i}}{\VecControl'} \cap \State{j} = \emptyset )
        \Big)
    \stopformula

    and used to generate the player 1 actions

    \startformula
        Act_1 = \BigSet{ \PlayerOneAction{i}{J} \Bigmid i \in I \MidComma J \subseteq I } \EndComma
    \stopformula

    named after the index of the origin state $\State{i}$ and the indices of reachable target states $\IndexedStates{j}{J}$, governed by $\sim_{\State{i}}$.
    No actions are defined for the outer states.
    Note that every set of reachable target states has a unique associated game action and control space region.
    The region is given by the concrete action dynamics operator $\ActC$ which reflects the action-generating relation $\sim_{\State {i}}$.

    For polytope $\State{4}$ of the example LSS (\in[fml:abstraction-example]) with the decomposition from Figure \in[fig:abstraction-graph-partition], the following non-empty concrete actions are computed:

    \startformula
        \startalign[n=2,align={left,left}]
            \NC \ConcreteAction{\State{4}}{\Set{\State{3},\State{4}}}
            \NC = \ClosedInterval{0.1}{1} \EndComma
            \NR
            \NC \ConcreteAction{\State{4}}{\Set{\State{1},\State{3},\State{4}}}
            \NC = \ClosedInterval{-0.1}{0.1} \EndAnd
            \NR
            \NC \ConcreteAction{\State{4}}{\Set{\State{1},\State{4}}}
            \NC = \ClosedInterval{-1}{-0.1} \EndPeriod
            \NR
        \stopalign
    \stopformula

    Therefore, 3 actions are derived for the associated player 1 state of the game graph: $\PlayerOneAction{4}{\Set{1, 4}}$, $\PlayerOneAction{4}{\Set{1, 3, 4}}$ and $\PlayerOneAction{4}{\Set{3, 4}}$.

\stopsubsection


\startsubsection[title={Player 2},reference=sec:abstraction-graph-playertwo]

    In the original LSS, after a control input has been selected, the next state is determined stochastically according to the evolution equation and the probability distribution over $\RandomSpace$.
    In the abstraction, a single probability distribution that describes the transitions of all $\VecState \in \State{i}$ does not exist generally, so the exact probabilities of reaching another target state when player 1 has selected an action are unknown until the trace and the control input are exactly localized.
    In particular, because of a possible mismatch between $\sim_\VecState$ and $\sim_{\State{i}}$, not every state in the action's target set may be reachable for every $\VecState \in \State{i}$.
    A simple probabilistic transition after a player 1 action to the next player 1 state is therefore not possible.
    The trace has to be localized in $\State{i}$ and the exact control input determined, so that the probability distribution over the set of target states is well defined for a transition.
    Hence, a second player with the power to localize the trace and select a specific control vector from the $\ActC$-regions associated with player 1 actions is required in the abstraction.
    The game can then transition to any of the reachable states by sampling the resulting probability distribution.

    Because every target set of state space partition elements has exactly one associated player 1 action, player 2 states

    \startformula
        G_2 = \BigSet{\Tuple{\State{i}}{J} \Bigmid i \in I \MidAnd J \subseteq I}
    \stopformula

    are simply defined as tuples of player 1 states and actions.
    The transition relation

    \startformula
        \Transition_\GameGraph
            \Big( \State{i}, \PlayerOneAction{i'}{J'} \Big)
            \Big( \Tuple{\State{i}}{J} \Big)
        = \startmathcases
            \NC 1
            \MC \startgathered
                    \NC \StartIf i = i' \MidAnd J = J'
                    \NR
                    \NC \quad \MidAnd \ConcreteAction{\State{i}}{\IndexedStates{j}{J}} \neq \emptyset
                    \NR
                \stopgathered
            \NR
            \NC 0
            \NC otherwise
            \NR
        \stopmathcases
    \stopformula

    matches each existing player 1 state-action pair with its corresponding player 2 state deterministically.

    Player 2's actions are supposed to determine the probability distribution of the actual system transition.
    A finite set of actions is desired but there are potentially infinitely many because the probability distributions will be different for every $\VecState \in \State{i}$ and $\VecControl \in \ConcreteAction{\State{i}}{\IndexedStates{j}{J}}$ that player 2 can choose from.
    Fortunately, in the context of almost-sure analysis, all non-zero probabilities can be considered equivalent.
    Intuitively this can be understood by considering repeated attempts to make some event happen.
    As long the probability of the event happening is non-zero in every attempt, it will happen eventually with probability 1, i.e.\ almost surely.
    The exact probability is irrelevant as long as it does not drop to 0.
    This property of almost-sure analysis reduces the choice of the probability distribution to a choice of the support set of the probability distribution.
    Therefore, (finitely many) player 2 actions

    \startformula
        Act_2 = \BigSet{ \PlayerTwoAction{i}{J}{K} \Bigmid i \in I \MidComma K \subseteq J \subseteq I } \EndComma
    \stopformula

    where $K$ is the set of target state indices from the support of the probability distribution, can be defined without losing relevant behaviour of the system for the purposes of almost-sure analysis.
    The equivalency of all probability distributions with the same support sets also means that, for convenience, uniform distributions can be chosen everywhere for the player 2 transition relation

    \startformula
        \Transition_\GameGraph
            \Big( \Tuple{\State{i}}{J}, \PlayerTwoAction{i'}{J'}{K} \Big)
            \Big( \State{k} \Big)
        = \startmathcases
            \NC \displaystyle\frac{1}{|K|}
            \MC \startgathered
                    \NC \StartIf i = i' \MidAnd J = J' \MidAnd k \in K
                    \NR
                    \NC \quad \MidAnd \PrecisePredecessor{\State{i}}{U_i^J}{\IndexedStates{k}{K}} \neq \emptyset
                    \NR
                \stopgathered
            \NR
            \NC 0
            \NC otherwise \EndComma
            \NR
        \stopmathcases
    \stopformula

    where $U_i^J = \ConcreteAction{\State{i}}{\IndexedStates{j}{J}}$ is the control input associated with the player 1 action $\PlayerOneAction{i}{J}$ that lead to the player 2 state $\Tuple{\State{i}}{J}$ from player 1 state $\State{i}$.
    The dynamics operator used to express the transition condition is the precise predecessor, which corresponds to the origin regions in $\State{i}$ for which probability distribution support sets $\IndexedStates{k}{K}$ are identical for some $\VecControl \in U_i^J$.

    In the example system (\in[fml:abstraction-example]), consider state $\State{4}$ and its action $\PlayerOneAction{4}{\Set{1, 3, 4}}$ computed in \in{Section}[sec:abstraction-graph-playerone].
    This action leads (determinisitically) to the player 2 state $\Tuple{\State{4}}{\Set{1, 3, 4}}$, which is associated with the non-empty precise predecessors

    \startformula
        \startalign[n=2,align={left,left}]
            \NC \PrecisePredecessor{\State{4}}{U_4^{\Set{1, 3, 4}}}{\Set{\State{1},\State{4}}}
            \NC = \ClosedInterval{0}{0.2} \EndComma
            \NR
            \NC \PrecisePredecessor{\State{4}}{U_4^{\Set{1, 3, 4}}}{\Set{\State{3},\State{4}}}
            \NC = \ClosedInterval{1.8}{2} \EndAnd
            \NR
            \NC \PrecisePredecessor{\State{4}}{U_4^{\Set{1, 3, 4}}}{\Set{\State{4}}}
            \NC = \ClosedInterval{0}{2} \EndPeriod
            \NR
        \stopalign
    \stopformula

    Therefore, in this player 2 state, the actions $\PlayerTwoAction{4}{\Set{1, 3, 4}}{\Set{1, 4}}$, $\PlayerTwoAction{4}{\Set{1, 3, 4}}{\Set{3, 4}}$ and $\PlayerTwoAction{4}{\Set{1, 3, 4}}{\Set{4}}$ are available.

\stopsubsection


\startreusableMPgraphic{example-play-paths}
    with spacing((45,60)) matrix.a(6,9);
    % Player 1 states (origin)
    with fixedboxwidth(40) with fixedboxheight(30) with shape(fixedbox) node.a[0][4](btex $\State{4}$ etex);
    % Player 2 states
    with fixedboxwidth(80) with fixedboxheight(30) with shape(fixedbox) with filling(solid) with fillingcolor(lightgray) node.a[2][1](btex $\Tuple{\State{4}}{\Set{1, 4}}$ etex);
    with fixedboxwidth(80) with fixedboxheight(30) with shape(fixedbox) with filling(solid) with fillingcolor(lightgray) node.a[2][4](btex $\Tuple{\State{4}}{\Set{1, 3, 4}}$ etex);
    with fixedboxwidth(80) with fixedboxheight(30) with shape(fixedbox) with filling(solid) with fillingcolor(lightgray) node.a[2][7](btex $\Tuple{\State{4}}{\Set{3, 4}}$ etex);
    % Player 1 states (target)
    with fixedboxwidth(40) with fixedboxheight(30) with shape(fixedbox) node.a[5][1](btex $\State{1}$ etex);
    with fixedboxwidth(40) with fixedboxheight(30) with shape(fixedbox) node.a[5][4](btex $\State{4}$ etex);
    with fixedboxwidth(40) with fixedboxheight(30) with shape(fixedbox) node.a[5][7](btex $\State{3}$ etex);
    % Player 1 actions
    with shape(circle) with size(35) node.a[1][1](btex \ssd $\Set{1, 4}$ etex);
    with shape(circle) with size(35) node.a[1][4](btex \ssd $\Set{1, 3, 4}$ etex);
    with shape(circle) with size(35) node.a[1][7](btex \ssd $\Set{3, 4}$ etex);
    % Player 2 actions
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][0](btex \ssd $\Set{1}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][1](btex \ssd $\Set{1, 4}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][2](btex \ssd $\Set{4}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][3](btex \ssd $\Set{1, 4}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][4](btex \ssd $\Set{4}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][5](btex \ssd $\Set{3, 4}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][6](btex \ssd $\Set{4}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][7](btex \ssd $\Set{3, 4}$ etex);
    with shape(circle) with filling(solid) with fillingcolor(lightgray) with size(35) node.a[3][8](btex \ssd $\Set{3}$ etex);
    % Arrows (player 1)
    with tipsize(0) arrow.top(.9, "") (a[0][4],a[1][1]) a[0][4].c..a[1][1].c;
    with tipsize(0) arrow.rt(.9, "") (a[0][4],a[1][4]) a[0][4].c..a[1][4].c;
    with tipsize(0) arrow.top(.9, "") (a[0][4],a[1][7]) a[0][4].c..a[1][7].c;
    arrow.rt(.5, "1") (a[1][1],a[2][1]) a[1][1].c..a[2][1].c;
    arrow.rt(.5, "1") (a[1][4],a[2][4]) a[1][4].c..a[2][4].c;
    arrow.rt(.5, "1") (a[1][7],a[2][7]) a[1][7].c..a[2][7].c;
    % Arrows (player 2)
    with tipsize(0) arrow.top(.9, "") (a[2][1],a[3][0]) a[2][1].c..a[3][0].c;
    with tipsize(0) arrow.top(.9, "") (a[2][1],a[3][1]) a[2][1].c..a[3][1].c;
    with tipsize(0) arrow.top(.9, "") (a[2][1],a[3][2]) a[2][1].c..a[3][2].c;
    with tipsize(0) arrow.top(.9, "") (a[2][4],a[3][3]) a[2][4].c..a[3][3].c;
    with tipsize(0) arrow.top(.9, "") (a[2][4],a[3][4]) a[2][4].c..a[3][4].c;
    with tipsize(0) arrow.top(.9, "") (a[2][4],a[3][5]) a[2][4].c..a[3][5].c;
    with tipsize(0) arrow.top(.9, "") (a[2][7],a[3][6]) a[2][7].c..a[3][6].c;
    with tipsize(0) arrow.top(.9, "") (a[2][7],a[3][7]) a[2][7].c..a[3][7].c;
    with tipsize(0) arrow.top(.9, "") (a[2][7],a[3][8]) a[2][7].c..a[3][8].c;
    arrow.rt(.6, btex $1$ etex) (a[3][0],a[5][1]) a[3][0].c..a[5][1].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][1],a[5][1]) a[3][1].c..a[5][1].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][1],a[5][4]) a[3][1].c..a[5][4].c;
    arrow.rt(.6, btex $1$ etex) (a[3][2],a[5][4]) a[3][2].c..a[5][4].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][3],a[5][1]) a[3][3].c..a[5][1].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][3],a[5][4]) a[3][3].c..a[5][4].c;
    arrow.rt(.6, btex $1$ etex) (a[3][4],a[5][4]) a[3][4].c..a[5][4].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][5],a[5][7]) a[3][5].c..a[5][7].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][5],a[5][4]) a[3][5].c..a[5][4].c;
    arrow.rt(.6, btex $1$ etex) (a[3][6],a[5][4]) a[3][6].c..a[5][4].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][7],a[5][7]) a[3][7].c..a[5][7].c;
    arrow.rt(.6, btex $\frac{1}{2}$ etex) (a[3][7],a[5][4]) a[3][7].c..a[5][4].c;
    arrow.rt(.6, btex $1$ etex) (a[3][8],a[5][7]) a[3][8].c..a[5][7].c;
\stopreusableMPgraphic

\startsubsection[title={Synopsis},reference=sec:abstraction-graph-synopsis]

    The constructed 2Â½-player game graph is

    \startformula
        \GameGraph = (G_1, G_2, Act, \Transition_\GameGraph) \EndComma
    \stopformula

    with $Act = Act_1 \cup Act_2$.

    To summarize, the game is played starting from a player 1 state as follows:
    First, player 1 selects a set of state-space partition elements reachable from the current state.
    One of these elements will be randomly selected as the next player 1 state.
    Because the real state of the trace in the LSS to which the play corresponds is unknown in the discretized state space, the distribution from which the successor is sampled is not uniquely defined yet and must be chosen by player 2.
    Conveniently, only the supports of the probability distributions matter for almost-sure analysis, so player 2 can choose from a (finite) set of uniform distributions with supports from the power set of the reachable states selected by player 1.

    Translated to the corresponding trace in the LSS:
    Player 1 chooses a region of the control space given by one of the non-empty concrete actions of the state space partition element the trace is currently located in.
    Player 2 then chooses a specific control vector from this region and reveals the exact location of the trace in the state space.
    The trace steps forward in time according to the evolution equation and it is player 1's turn again.

    \placefigure[top][fig:abstraction-graph-xfour]{
        A subset of the game graph abstraction for the example system (\in[fml:abstraction-example]) with the partition from Figure \in[fig:abstraction-graph-partition].
        Rectangles denote states and circles denote actions.
        Player 1's states and actions are coloured white, player 2's grey.
        The probabilistic transitions after actions are labeled with their respective transition probabilities.
        The two depicted states $\State{4}$ are the same and only shown separately to improve readability.
    }{
        \framed[width=\textwidth,frame=off]{\reuseMPgraphic{example-play-paths}}
    }

    An excerpt from the game graph constructed for the example system (\in[fml:abstraction-example]) is shown in Figure \in[fig:abstraction-graph-xfour].
    It shows all actions for the player 1 state $\State{4}$ (computed in \in[sec:abstraction-graph-playerone]) as well as all player 2 successor states and their actions (partially computed in \in[sec:abstraction-graph-playertwo]).

    % TODO: show a trace starting in ambiguous PreP and explain player 2 selections for both actions

\stopsubsection

