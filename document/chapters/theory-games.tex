If a system of interest exhibits stochastic behaviour, there is a need for probabilistic abstractions that can reflect this stochasticity appropriately.
Markov models and the closely related probabilistic games are such models for systems with discrete time evolution.
A few classes of these models are introduced in this section.


\startsubsection[title={Markov Models}]

    Markov models are transition systems enriched with probabilistic behaviour and have found numerous applications, including in probabilistic model checking \cite[alternative=authoryears,left={(see e.g. chapter 10 of }][Baier2008]. % TODO reference one or two papers as well

    The simplest Markov model is the Markov chain (MC).
    A Markov chain is a tuple $\Tuple{G}{\delta}$ where $G$ is a set of states and $\Function{\delta}{G}{\ProbDist{G}}$ a transition relation with $\ProbDist{G}$ denoting the set of all probability distributions over $G$.
    At every step of a trace through the MC, a successor is chosen by sampling the probability distribution of successors of the current state defined by $\delta$.

    Markov decision processes (MDPs) extend MCs by introducing actions.
    MDPs are 3-tuples $\Triple{G}{Act}{\delta}$, where $G$ is a set of states, $Act$ is a set of actions and $\Function{\delta}{G \times Act}{\ProbDist{G}}$ is a probabilistic transition relation.
    At every step of a trace through the MDP, an action is chosen and the successor state determined by sampling the probability distribution associated with the current state and chosen action through $\delta$.
    Not all actions are enabled in every state, but at least one action from $Act$ must be.
    A Markov chain is the degenerate case of a MDP with exactly one action enabled in every state.

    The introduction of actions with the MDP opens up a game-theoretic perspective on Markov models.
    From the perspective of games, the transition system of an MDP is a game graph on which a turn-based game is played between a player and a probabilistic environment.
    The player chooses their move each turn by picking an action and the outcome is determined stochastically by the environment, which acts as a sort of passive player.
    MDPs are therefore also called 1-player probabilistic games or 1½-player games, with one \quotation{proper} player and the environment in a player-like role.
    This game-based view of Markov models extends naturally to more complicated behaviour through the introduction of additional players.

\stopsubsection


\startsubsection[title={2-Player Probabilistic Games},reference=sec:theory-games-games]

    A two-player probabilistic game, or 2½-player game, is a turn-based probabilistic game played on a game graph

    \startformula
        \mathcal{G} = (G_1, G_2, Act, \delta) \EndComma
    \stopformula

    where $G_1$, $G_2$ are disjoint sets of player 1 and 2 states, respectively.
    $Act$ and $\delta$ are defined as for the MDP with $G = G_1 \cup G_2$.
    A play is a sequence of states $g = g_0 g_1 ... \in G^\omega$ such that $g_i \in G_1$ for all even and $g_i \in G_2$ for all odd indices $i$, i.e. player's turns alternate beginning with player 1. % TODO ^\omega notation not introduced yet
    In their turn, players choose an available action from the finite set of actions $Act$.
    The next state of the play is then chosen stochastically based on the probability distribution defined by the probability transition function $\delta: G \times Act \rightarrow \mathcal{D}(G)$.
    Therefore, for every $g_i$ there must be an action $a \in Act$ such that $\delta(g_i, a)(g_{i+1}) \gt 0$.

    A player $k$ strategy is a function $\Strategy{k}{\GameGraph}: G^+ \rightarrow Act$ that determines the action taken after a finite prefix of a play ending in a state of player $k$.
    A strategy requiring only a fixed-size prefix is called finite-memory and one using only the current game state to select an action is called memoryless.

\stopsubsection


\startsubsection[title={Winning and Solving 2-Player Probabilistic Games}]

    A notion of winning is introduced by extending the game graph $\mathcal{G}$ with an acceptance condition $\mathcal{C}$ to form the game

    \startformula
        \mathcal{G'} = (G_1, G_2, Act, \delta, \mathcal{C}) \EndPeriod
    \stopformula

    The acceptance condition separates the set of all possible plays into those which are won by player 1 and those plays are won by player 2.
    Every play has exacly one winning player, no plays are won by neither or both players.

    The solution of a game are the sets of initial states for which player 1 (2) has a winning strategy, i.e. a strategy that ensures a win of player 1 (2).
    The meaning of \quotation{ensuring a win} depends on the type of analysis carried out. % TODO phrasing
    In this work, (qualititive) almost-sure analysis for and adversarial and cooperative player 2 is the main concern.
    In almost-sure analysis, game states are sought for which a winning strategy exists for one player, ensuring a win with probability 1. % TODO phrasing
    Adversarial analysis means that the player strategy must lead to a win for every possible strategy of the opponent player, while in a cooperative setting a winning strategy only has to exist for some strategy of the other player, i.e. a win is possible but only ensurable if the opponent is \quotation{nice} and cooperates.
    In the cooperative interpretation, the 2 player probabilistic game can be reduced to a 1 player probabilistic game \cite[authoryears][Svorenova2017].
    The set of initial states for which player 1 has a strategy to ensure winning almost surely are denoted in the adversarial setting by $\Almost{\GameGraph}(\mathcal{C})$ and in the cooperative setting as $\AlmostCoop{\GameGraph}(\mathcal{C})$.

    % TODO transition, variety of ways to express condition analogously to automata

\stopsubsection

