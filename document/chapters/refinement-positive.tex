Actively enable good things, reaching of target.
Remember, ensuring good things is non-trivial, because any bad event with non-zero probability breaks everything.

Approach here: based on refinement decomposition.
Definition a positive refinement kernel

\startformula
    \Function{\RefinePos}{C_n \times R_n}{2^{C_n}}
\stopformula

which partitions a polytope $X$ into convex polytopes such that afterwards $Y$ can be reached for some parts of the partition or ideally every part ($R_n$ and $C_n$ are defined as in the chapter introduction).
Kernel only looks at single step of a trace, i.e.\ a player 1 turn followed by a player 2 turn in the game.
Using the kernel, complex methods that make use of larger patterns in the game can be constructed.

Much attention is given to the robust predecessor of target region as it returns region where deterministic transitions are possible for which guarantees are easy to obtain.
Ignoring the probabilistic dynamics will mean that not all problems can be solved in a straightforward manner using only deterministic approaches and it is likely that refinement with the goal of creating deterministic transitions leads to partitions that are more complex than they need to be.


\startsubsection[title={Positive Robust Attractor}]

    AttrR+, by \cite[Svorenova2017]
    Single-step lookahead.
    Progress guarantees, allows cheap analysis for reachability problems.
    AttrR+ treats system like it is deterministic.
    Waste of probabilistic properties.
    Non-convex target regions can lead to explosion of complexity.
    No progress possible if deterministic transition cannot be found, approximations do not deliver same guarantees.
    Generally, an existing yes region is not available.

    Coordinated application:
    Refine every state in PreR of target region using AttrR+ refinement.
    Use progress guarantee to immediately extend region of yes-states.
    Iterate.
    Multiple iterations possible without need to analyse full system (expensive!), but probabilistic behaviour ignored.
    Essentially the procedure that \cite[Svorenova2017] used in their case study.

\stopsubsection


\startsubsection[title={Control Input Selection}]

    AttrR+ requires control input to refine wrt.

    \cite[Svorenova2017] selected this by taking a random action, paritioning it arbitrarily and using the parts.
    From experience: this produces small states, progress is slower than it should be.
    Improvement: rate action by how much its posterior intersects target/no-region/etc, then pick "best" one.
    Do the same with parts.
    However, this still leaves room for improvement.

    Game-based vs. geometric condition: game-based requires player 1 actions to be available (if all contain at least one no state in target set, condition is met).
    While this is always the case after an analysis, for which the game is constructed, the condition can be expensive to evaluate if multiple refinement steps are taken between analysis because refinement invalidates the previous game graph at least partially.
    Action construction for both players is of exponential complexity in number of reachable states and involves many geometric operations.
    Alternative: purely geometric condition, use dynamics to detect situation that requires refinement without requiring the game graph.
    Insight is somewhat restricted but condition might be much cheaper to evaluate with only a few geometric operations and polynomial complexity in the number of reachable states.

    Alternative: Monte Carlo-based (geometric) approach
    Sample a few points from within the polytope.
    Use $ActR$ to find control inputs for those points that exclusively lead to target (if there are none, try another sample point).
    Then cluster these control inputs and select one that most points "agree" with.
    Expensive but exhaustive: compute all overlaps, similar to action computation and pick one with most contributung points.
    In practice, adding vertices to sample point pool improved performance, as these points are often associated with most "extreme" action requirements.

\stopsubsection


\startsubsection[title={Target Preprocessing}]

    Small targets that cannot be transitioned to exclusively are a problem for deterministic refinement methods such as AttrR+.
    Even regions that can be targeted individually but that are very small are a problem, as a sufficiently big target area has to be built up first, before well-performing refinement is possible.
    Due to probabilistic nature this is not always optimal.
    Target may be enlarged in these cases, carefully, so that probability of going to "real" target is non-zero everywhere in extended region and extended region cannot be targeted individually either.
    Extended region may require refinement itself, e.g. saftey refinement wrt entire extended region.

    Linear dynamics means that convex targets usually have convex reachability solutions (due to $\Pre$ being convex if state, action and random space are convex).
    Non-convex targets often introduce more non-convexity which leads to state explosion.
    Target approximation can work by hoping for probabilisitic "magic" but hard to predict when this works and when it doesn't.
    Smoothing of target region, up to taking convex hull.

\stopsection


\startsubsection[title={Layered Refinement}]

    Problem with pure AttrR+ refinement is that progress guarantee only works for AttrR-generated states.
    For some, reachability can be decided with ActR but purely deterministic view means over-refinement or conservative analysis, slowing progress.
    Analysing system from time to time required, but relatively expensive due to need for action computation.

    Layer idea outlined by \cite[Svorenova2017].
    Refinement decouples into separate subproblems.
    Problem of \cite[Svorenova2017]: Pre is not the ideal operator for procedure.

    Innovation: PreR, shrinking of layer-generating control space (show graphic/example that illustrates "convergence" behaviour at edges).
    Outline algorithm (layer generation, removal of known no-states, control selection and AttrR in inner iterations, small-state supression because states smaller W will always transition away, loops impossible), discuss tuning parameters.
    Discuss probabilistic aspect of approach (steps are deterministic, but no overall guarantee since solution for outer layers depends on solution of inner layers) but mention that at core it is deterministic.
    Hybrid approach between multi-step and single-step: general idea of moving from layer to layer is multi-step, but transitions inside layers are single step.
    Non-optimality of layers in terms of numbers of states generated (PreR is deterministic transition).
    Problem again: PreR does not exists if X - W = 0, extension by minkowski sum with origin-centered W and additional extension when target at the edge of the control space.

\stopsubsection


\startsubsection[title={Reduction Approaches}]

    Game-graph construction expensive, independent of objective.
    Deterministic vs probabilistic refinement, missed potential but cost of deeper analysis required in probabilistic approaches.
    Single-step vs. multi-step refinement and hybrid approaches.
    Static control reduces system by removing player 1 and therefore enabling more efficient analysis of system at the cost of introduction of a piecewise dynamics.
    Discuss theoretical successes and practical problems of static control.

    Discuss purely deterministic refinement and refinement that takes probabilistic aspect into account.
    "Jagged" progress due to piecewise dynamics lead no much non-convexness which is computationally demanding.
    More sophisticated action selection could resolve this, but fixed dynamics without recomputation after some steps will always have its limits.

\stopsubsection

