Consider the double integrator dynamics

\startformula
    \VecX_{t+1} = \TwoByTwo{1}{1}{0}{1} \VecState_{t} + \TwoByOne{0.5}{1} \VecControl_{t} + \VecRandom_{t} \EndComma
\stopformula

where $\VecState_{t} \in \StateSpace = \ClosedInterval{-5}{5} \times \ClosedInterval{-3}{3}$, $\VecControl_{t} \in \ControlSpace = \ClosedInterval{-1}{1}$ and $\VecRandom_{t} \in \RandomSpace = \ClosedInterval{-0.1}{0.1}^2$.
The objective is to reach $\ClosedInterval{-1}{1}^2$, therefore the linear predicates $\Predicate_{1}$, $\Predicate_{2}$, $\Predicate_{3}$ and $\Predicate_{4}$ are introduced, corresponding to halfspaces governed by the inequalities $x \leq 1$, $-x \leq 1$, $y \leq 1$ and $-y \leq 1$.
The LTL formula expressing this reachability objective is then

\startformula
    \Finally ( p_1 \wedge p_2 \wedge p_3 \wedge p_4 )
\stopformula

with a co-safe interpretation.
\cite[Svorenova2017] used this exact system for their case studies and it is used here again for a detailed exploration of the robust refinement methods.

\placefigure[top][fig:cases-integrator-initial]{
    The double integrator test system and its initial partition.
    Polytopes $\State{1}$ to $\State{4}$ (grey) are outer states from the decomposition of $\ExtendedStateSpace \setminus \StateSpace$.
    The reachability target is $\State{12}$ (green).
}{
    \externalfigure[cases-integrator-initial][width=\textwidth]
}


\startsubsection[title={Negative Refinement},reference=sec:cases-integrator-negative]

    \placetable[top][tab:cases-integrator-negative]{
        TODO \par TODO
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=1,polys=9,onestates=28,oneactions=44,twostates=26,twoactions=216,
                                total={0:00},refinement={-},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=0.0,maybe=93.3,figure=cases-integrator-iteration1]
            \RefinementTableRow[iteration=2,polys=13,onestates=36,oneactions=60,twostates=38,twoactions=362,
                                total={0:00},refinement={0:00},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=11.3,maybe=82.1,figure=cases-integrator-iteration2]
            \RefinementTableRow[iteration=3,polys=17,onestates=44,oneactions=80,twostates=50,twoactions=592,
                                total={0:01},refinement={0:00},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=16.1,maybe=77.3,figure=cases-integrator-iteration3]
            \RefinementTableRow[iteration=4,polys=21,onestates=52,oneactions=104,twostates=66,twoactions=794,
                                total={0:01},refinement={0:00},gamegraph={0:00},analysis={0:00},
                                yes=6.7,no=17.1,maybe=76.2,figure=cases-integrator-iteration4]
        }
    }

    Begin by removing negative attractor sets, here of the outer region.
    Iterate until convergence, here 3 steps.
    Introduction into refinement progress reporting.

    System analysis is not set up to profit from the progress guarantee, analysis constructs game based on the previous results only.
    Relying on progress guarantee of robust states alone neglects probabilistic aspects of dynamics and iteraction with other states generated in partitioning.

    Show that simplification with multiple iterations of negative attractor does not generally work because of self-loops.

\stopsubsection



\startsubsection[title={Positive Robust Refinement},reference=sec:cases-integrator-positive]

    \placetable[top][tab:cases-integrator-positive-single]{
        TODO \par TODO
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=48,onestates=106,oneactions=476,twostates=407,twoactions=6188,
                                total={0:04},refinement={0:00},gamegraph={0:03},analysis={0:00},
                                yes=13.9,no=17.1,maybe=68.9,figure=cases-integrator-iteration5-prs]
            \RefinementTableRow[iteration=6,polys=166,onestates=342,oneactions=3723,twostates=3519,twoactions=79332,
                                total={2:07},refinement={0:02},gamegraph={1:59},analysis={0:03},
                                yes=36.5,no=17.1,maybe=46.4,figure=cases-integrator-iteration6-prs]
            \RefinementTableRow[iteration=7,polys=301,onestates=612,oneactions=5902,twostates=5459,twoactions=111705,
                                total={5:59},refinement={0:09},gamegraph={3:40},analysis={0:03},
                                yes=54.9,no=17.1,maybe=28.0,figure=cases-integrator-iteration7-prs]
            \RefinementTableRow[iteration=8,polys=362,onestates=734,oneactions=3096,twostates=2458,twoactions=54578,
                                total={9:41},refinement={0:07},gamegraph={3:33},analysis={0:02},
                                yes=67.0,no=17.1,maybe=15.9,figure=cases-integrator-iteration8-prs]}
    }

    \placetable[top][tab:cases-integrator-positive-double]{
        TODO \par TODO
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=68,onestates=146,oneactions=885,twostates=796,twoactions=13251,
                                total={0:11},refinement={0:00},gamegraph={0:10},analysis={0:00},
                                yes=18.4,no=17.1,maybe=64.5,figure=cases-integrator-iteration5-prm]
            \RefinementTableRow[iteration=6,polys=236,onestates=482,oneactions=6293,twostates=6003,twoactions=141782,
                                total={4:32},refinement={0:06},gamegraph={4:08},analysis={0:06},
                                yes=53.9,no=17.1,maybe=29.0,figure=cases-integrator-iteration6-prm]
            \RefinementTableRow[iteration=7,polys=333,onestates=676,oneactions=4114,twostates=3567,twoactions=53441,
                                total={6:09},refinement={0:06},gamegraph={1:29},analysis={0:02},
                                yes=77.0,no=17.1,maybe=5.9,figure=cases-integrator-iteration7-prm]
            \RefinementTableRow[iteration=8,polys=361,onestates=732,oneactions=1367,twostates=682,twoactions=7436,
                                total={6:18},refinement={0:01},gamegraph={0:08},analysis={0:00},
                                yes=82.5,no=17.1,maybe=0.3,figure=cases-integrator-iteration8-prm]
        }
    }

    \placetable[top][tab:cases-integrator-positive-suppressed]{
        TODO \par TODO
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=51,onestates=112,oneactions=507,twostates=435,twoactions=6147,
                                total={0:05},refinement={0:00},gamegraph={0:04},analysis={0:00},
                                yes=18.5,no=17.1,maybe=64.3,figure=cases-integrator-iteration5-prms]
            \RefinementTableRow[iteration=6,polys=99,onestates=208,oneactions=1360,twostates=1221,twoactions=21859,
                                total={0:26},refinement={0:02},gamegraph={0:18},analysis={0:01},
                                yes=42.4,no=17.1,maybe=40.4,figure=cases-integrator-iteration6-prms]
            \RefinementTableRow[iteration=7,polys=134,onestates=278,oneactions=1291,twostates=1069,twoactions=16174,
                                total={0:41},refinement={0:02},gamegraph={0:13},analysis={0:00},
                                yes=65.6,no=17.1,maybe=17.3,figure=cases-integrator-iteration7-prms]
            \RefinementTableRow[iteration=8,polys=161,onestates=332,oneactions=962,twostates=671,twoactions=8157,
                                total={0:48},refinement={0:01},gamegraph={0:05},analysis={0:00},
                                yes=80.1,no=17.1,maybe=2.7,figure=cases-integrator-iteration8-prms]
        }
    }

    Recreation of Svorenova positive refinement procedure, who refined wrt 3 random actions.
    Control selection for $\RefinePos$ is generally better than action-based, therefore, two single-step applications here.
    Second iteration introduces much small-scale jaggedness in upper left and lower right regions.
    Asymmetry of partitioning of the rest is an artifact of the geometry implementation.
    Here, it works in favour of lower right region.
    Polygon count exceeds 300 after 3 applications of positive refinement.
    After 4 applications, 84\% of state space is decided as either belonging to $\InitialStates$ or $\StateSpace \setminus \InitialStates$.

    Transition decomposition allows taking multiple robust steps.
    Problem is already reachability and co-safe, therefore extraction of robust reachability problem trivial (transition $q_0$ to $q_1$).
    Positive robust refinement, two iterations per step, with expanding target region based on robust reachability.
    As expected, after one application more progress than single-step scheme.
    Second iteration with gain of 168 polytopes and correspondingly an expensive game graph construction (almost twice the size of single-step iteration 2).
    Progress after analysis however comparable to 3rd iteration of single-step.
    Game simplification then allows faster analysis of remaining two iterations with very fast progress despite similar numbers of polytopes in the partition.
    State space pretty much completely decided after 4 applications while elapsed time more than 3 minutes less than for single-step.

    Again, for this specific example where target region is known to be satisfying, additional performance could be gained from using the robust progress guarantee.
    Game construction is by far the bottleneck and reducing the number of polytopic operations required would boost performance \.
    This is however not generally applicable, special case here for co-safe objective.
    To not misrepresent the results for general applications, progress guarantee transfer has not been implemented.

    Same setup as two-step but with post-processing of $\RefinePos$ such that all polytopes $Y$ where $Y \ominus \RandomSpace$ are removed (compare with \in{Figure}[fig:refinement-transition-jagged]d).
    Table \in[TODO] shows that progress in each iteration is slowed but state space explosion is significantly reduced.
    Similar overall progress to method without post-processing reached in a tenth of the time, thanks to massively reduced cost of game construction.
    Similar partitioning if 8 iterations are carried out directly (not shown).
    Overall time takes longer due to intermediate game simplification.
    Post-processing with overapproximation using convex hull yielded unstable results.
    While overapproximation would sometimes work out and system would progress, it sometimes got stuck due to the lost robustness guarantees, that could not be compensated for by the probabilistic dynamics of the LSS.

\stopsubsection


\startsubsection[title={Positive Robust Refinement with Layer Decomposition},reference=sec:cases-integrator-layered]

    \placetable[top][tab:cases-integrator-layered-original]{
        TODO \par TODO
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=166,onestates=342,oneactions=4364,twostates=4177,twoactions=79056,
                                total={1:24},refinement={0:03},gamegraph={1:16},analysis={0:05},
                                yes=76.3,no=17.1,maybe=6.6,figure=cases-integrator-iteration5-lprms]
        }
    }

    Application of additional layer decomposition using the $\PreR$ operator as layer generator.
    Due to previous success, use small state suppression post-processing and robust expansion of target region again.
    Outer layers depend on solution of inner layers, therefore number of iterations of robust refinement applied in each layer raised to 4 to increase likelyhood of solution.

    \placefigure[top][fig:cases-integrator-layers]{
        TODO
    }{
        \bTABLE[offset=0mm,frame=off]
            \bTR[boffset=4mm]
                \bTD \externalfigure[cases-integrator-layers-prer100][width=0.494\textwidth] \eTD
                \bTD \externalfigure[cases-integrator-layers-prer95][width=0.494\textwidth] \eTD
            \eTR
            \bTR
                \bTD[nc=2] \externalfigure[cases-integrator-layers][width=\textwidth] \eTD
            \eTR
        \eTABLE
    }

    First with $\PreR$, one application with all layers ($\PreR$ converges after TODO applications) yields a partitioning of similar number of states as final iteration of two-step robust scheme with suppression.
    No intermediate game simplification possible, therefore larger game has to be constructed and performance falls short.
    One issue that increases the polytope count is that the $PreR$-regions exhibit \epsilon-limit behaviour.
    This causes small, thin states to appear at its edges where a robust transition cannot be guaranteed as a single-vector control input is required which is not representable in the chosen abstraction model.
    Appearance of such states cannot be suppressed as they are not part of the $\AttrR$ region of $\RefinePos$ but are created when partitioning the remaining region of the refined polytope.
    Compare with partitioning when generating control region is shrunk by 95\% ($\ClosedInterval{-0.95}{0.95}$ instead of $\ClosedInterval{-1}{1}$).
    Limit behaviour completely eliminated, at the cost of only a little less layer size (gap between negative attractor region and outermost layer.
    Performance with shrunk control region exceeds that of two-step with intermediate analysis despite having to construct the entire game.
    Lowest number of states seen and only 1\% of state space volume remaining undecided.
    For comparison, minimum number of states that arise just from the layer decomposition alone is 106.
    Less states is impossible, and refinement here shows need for only 23 additional states.

    \placetable[top][tab:cases-integrator-layered-shrunk]{
        TODO \par TODO
    }{
        \RefinementTable{
            \RefinementTableRow[iteration=5,polys=129,onestates=268,oneactions=2565,twostates=2415,twoactions=36246,
                                total={0:29},refinement={0:01},gamegraph={0:25},analysis={0:02},
                                yes=81.8,no=17.1,maybe=1.0,figure=cases-integrator-iteration5-slprms]
        }
    }

\stopsubsection


\startbuffer[buf:cases-integrator-results-statistics]
    \setupTABLE[frame=off,rightframe=on]
    \setupTABLE[c][first][rightframe=off]
    \setupTABLE[r][last][bottomframe=on]
    \bTABLE[align={middle,lohi}]
        \bTR[topframe=off]
            \bTH[width=0.06\textheight] \eTH
            \bTH[width=0.09\textheight] \eTH
            \bTH[width=0.17\textheight] single-step \times 2 \eTH
            \bTH[width=0.17\textheight] two-step \eTH
            \bTH[width=0.17\textheight] two-step \par no small \eTH
            \bTH[width=0.17\textheight] layers \par four-step \par no small \eTH
            \bTH[width=0.17\textheight] shrunk layers \par four-step \par no small \eTH
        \eTR
        \bTR[topframe=off]
            \bTH Iter. \eTH
            \bTH \eTH
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
            \bTD min / avg / max \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 1-4 \par neg. \eTH
            \bTD polytopes \eTD
            \bTD[nc=5] 21 / 21 / 21 \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD[nc=5] 0:01 / 0:01 / 0:01 \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD[nc=5] 23.8 / 23.8 / 23.8 \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 5 \eTH
            \bTD polytopes \eTD
            \bTD 48 / 50 / 53 \eTD
            \bTD 68 / 85 / 97 \eTD
            \bTD 49 / 54 / 60 \eTD
            \bTD 165 / 168 / 171 \eTD
            \bTD 128 / 130 / 133 \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 0:04 / 0:04 / 0:05 \eTD
            \bTD 0:11 / 0:21 / 0:36 \eTD
            \bTD 0:04 / 0:05 / 0:07 \eTD
            \bTD 1:19 / 1:24 / 1:27 \eTD
            \bTD 0:28 / 0:29 / 0:31 \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 30.7 / 31.0 / 31.4 \eTD
            \bTD 35.5 / 38.4 / 39.6 \eTD
            \bTD 35.5 / 36.7 / 39.2 \eTD
            \bTD 93.4 / 95.0 / 96.7 \eTD
            \bTD 99.0 / 99.0 / 99.0 \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 6 \eTH
            \bTD polytopes \eTD
            \bTD 166 / 194 / 239 \eTD
            \bTD 227 / 277 / 317 \eTD
            \bTD 83 / 99 / 114 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 1:55 / 4:00 / 11:56 \eTD
            \bTD 2:16 / 6:06 / 16:36 \eTD
            \bTD 0:18 / 0:25 / 0:36 \eTD
            \bTD - \eTD
            \bTD - \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 53.6 / 55.8 / 58.5 \eTD
            \bTD 70.7 / 74.8 / 79.4 \eTD
            \bTD 53.6 / 61.8 / 71.0 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3] 7 \eTH
            \bTD polytopes \eTD
            \bTD 239 / 280 / 321 \eTD
            \bTD 300 / 353 / 394 \eTD
            \bTD 124 / 132 / 137 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 4:04 / 7:14 / 17:13 \eTD
            \bTD 3:10 / 7:24 / 17:59 \eTD
            \bTD 0:26 / 0:41 / 0:50 \eTD
            \bTD - \eTD
            \bTD - \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 69.3 / 72.0 / 73.3 \eTD
            \bTD 87.8 / 95.6 / 99.5 \eTD
            \bTD 76.2 / 83.8 / 89.8 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR[topframe=on]
            \bTH[nr=3,bottomframe=on] 8 \eTH
            \bTD polytopes \eTD
            \bTD 296 / 351 / 408 \eTD
            \bTD 333 / 389 / 435 \eTD
            \bTD 141 / 151 / 163 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
        \bTR
            \bTD elapsed \eTD
            \bTD 6:15 / 12:35 / 27:24 \eTD
            \bTD 3:26 / 8:53 / 18:07 \eTD
            \bTD 0:31 / 0:47 / 0:58 \eTD
            \bTD - \eTD
            \bTD - \eTD
        \eTR
        \bTR
            \bTD \% decided \eTD
            \bTD 81.6 / 84.3 / 85.6 \eTD
            \bTD 95.5 / 99.2 / 99.8 \eTD
            \bTD 89.9 / 96.5 / 99.3 \eTD
            \bTD  \eTD
            \bTD  \eTD
        \eTR
    \eTABLE
\stopbuffer

\startsubsection[title={Results},reference=sec:cases-integrator-results]

    % Rotation value has to be flipped for some reason (maybe because it refers
    % to the page that includes the figure, not the figure itself?
    % % TODO check rotation in final document
    \placetable[here,\doifoddpageelse{270}{90}][tab:cases-integrator-results-statistics]{
        TODO
    }{
        \startframedtext[width=\textheight,offset=0mm,frame=off,topframe=off,bottomframe=off]
            \getbuffer[buf:cases-integrator-results-statistics]
        \stopframedtext
    }

    Show table with average numbers for all problems.
    Talk about randomization, stability of performance results, speed of progress.
    Layered refinement performs best, importance of small-state suppression (combats over-refinement) and problems with jaggedness (which just causes more jaggedness).
    Plot number of state space partition elements vs. game size for full analysis.
    Show how product game simplification can save time but that step-wise expansion of yes-region is generally not possible.

    3 additional discarded runs of single-step due to numerical instability when very small states appeared.
    Failure when calculating PreP regions.
    Unknown if due to limitation of implemented geometry library or unavoidable.
    Countermeasure taken don't refine very small states further but they still can appear as part of the refinement partitioning.
    Would be possible to refuse partitioning if small states appear at the cost of progress.
    During development and testing no problems occurred when small state suppression was used, problem limited to small scale geometry.

\stopsubsection

